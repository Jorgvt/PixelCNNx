{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation\n",
    "\n",
    "> Here we'll be following Keras' example: https://keras.io/examples/generative/pixelcnn/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-08 23:56:26.086253: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-08-08 23:56:26.142007: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-08 23:56:27.875833: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.config.set_visible_devices([], device_type='GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Any, Callable, Sequence, Union\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import jax\n",
    "from jax import lax, random, numpy as jnp\n",
    "from flax.core import freeze, unfreeze, FrozenDict\n",
    "from flax import linen as nn\n",
    "from flax import struct\n",
    "from flax.training import train_state\n",
    "from flax.training import orbax_utils\n",
    "\n",
    "import optax\n",
    "import orbax.checkpoint\n",
    "\n",
    "from clu import metrics\n",
    "from ml_collections import ConfigDict\n",
    "\n",
    "from einops import reduce\n",
    "import wandb"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "We'll be using the MNIST dataset in order to simplify the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 28, 28, 1), (10000, 28, 28, 1))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(X_train, _), (X_test, _) = mnist.load_data()\n",
    "\n",
    "X_train = X_train[:,:,:,None]\n",
    "X_test = X_test[:,:,:,None]\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In their example they binarize the images. We'll follow along but I don't know why are they doing this yet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([0., 1.], dtype=float32, weak_type=True)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_bin = jnp.where(X_train < (0.33)*256, 0., 1.)\n",
    "X_test_bin = jnp.where(X_test < (0.33)*256, 0., 1.)\n",
    "\n",
    "# X_train_bin = X_train_bin.astype(jnp.float32)\n",
    "# X_test_bin = X_test_bin.astype(jnp.float32)\n",
    "\n",
    "jnp.unique(X_train_bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dst_train = tf.data.Dataset.from_tensor_slices(X_train_bin)\n",
    "dst_val = tf.data.Dataset.from_tensor_slices(X_test_bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaEAAAGdCAYAAAC7EMwUAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAXvUlEQVR4nO3df2hV9/3H8ddV410qNxeCTe69Mw1hKB1GHFOnBn9EmZn3y0SbDmwLI8Im7RqFkJYy5x+G/WGKw+AfWR0rwynT6T9WBaU2QxMnzpGK0uCKpBhnhrkEQ3tvTN3V1M/3j3y9316TxuZ6b9659z4fcMB77on37fHQZ4/35hOPc84JAAAD06wHAADkLyIEADBDhAAAZogQAMAMEQIAmCFCAAAzRAgAYIYIAQDMzLAe4EmPHj3SnTt35PP55PF4rMcBAEyQc06Dg4MKhUKaNm38e50pF6E7d+6orKzMegwAwDPq7e3VnDlzxj1mykXI5/NJklbofzRDBcbTAAAmalgPdVFnEv89H0/GIvTee+/pd7/7nfr6+jR//nzt27dPK1eufOrXPf4nuBkq0AwPEQKArPN/K5J+m7dUMvLBhGPHjqmhoUE7d+7U1atXtXLlSoXDYd2+fTsTLwcAyFIZiVBLS4t+8Ytf6Je//KW+//3va9++fSorK9P+/fsz8XIAgCyV9gg9ePBAV65cUU1NTdL+mpoaXbp0adTx8XhcsVgsaQMA5Ie0R+ju3bv66quvVFpamrS/tLRUkUhk1PHNzc3y+/2JjU/GAUD+yNg3qz75hpRzbsw3qXbs2KFoNJrYent7MzUSAGCKSfun42bPnq3p06ePuuvp7+8fdXckSV6vV16vN91jAACyQNrvhGbOnKlFixapra0taX9bW5uqqqrS/XIAgCyWke8Tamxs1M9//nMtXrxYy5cv1x//+Efdvn1bb7zxRiZeDgCQpTISoc2bN2tgYEC//e1v1dfXp8rKSp05c0bl5eWZeDkAQJbyOOec9RBfF4vF5Pf7Va2NrJgAAFlo2D1Uu04qGo2qqKho3GP5UQ4AADNECABghggBAMwQIQCAGSIEADBDhAAAZogQAMAMEQIAmCFCAAAzRAgAYIYIAQDMECEAgBkiBAAwQ4QAAGaIEADADBECAJghQgAAM0QIAGCGCAEAzBAhAIAZIgQAMEOEAABmiBAAwAwRAgCYIUIAADNECABghggBAMwQIQCAGSIEADBDhAAAZogQAMAMEQIAmCFCAAAzRAgAYIYIAQDMECEAgBkiBAAwQ4QAAGaIEADADBECAJghQgAAM0QIAGCGCAEAzBAhAIAZIgQAMEOEAABmiBAAwAwRAgCYIUIAADNECABghggBAMwQIQCAmRnWAwDInLN3rlmPMK6fhH4w4a+Z6n+mVKRyHnIFd0IAADNECABgJu0RampqksfjSdoCgUC6XwYAkAMy8p7Q/Pnz9be//S3xePr06Zl4GQBAlstIhGbMmMHdDwDgqTLynlB3d7dCoZAqKir0yiuv6ObNm994bDweVywWS9oAAPkh7RFaunSpDh06pLNnz+r9999XJBJRVVWVBgYGxjy+ublZfr8/sZWVlaV7JADAFJX2CIXDYb388stasGCBfvzjH+v06dOSpIMHD455/I4dOxSNRhNbb29vukcCAExRGf9m1VmzZmnBggXq7u4e83mv1yuv15vpMQAAU1DGv08oHo/r008/VTAYzPRLAQCyTNoj9Pbbb6ujo0M9PT365z//qZ/97GeKxWKqq6tL90sBALJc2v857j//+Y9effVV3b17V88//7yWLVumy5cvq7y8PN0vBQDIcmmP0NGjR9P9WyLP5eKClRiRi3+3+bwYaSpYOw4AYIYIAQDMECEAgBkiBAAwQ4QAAGaIEADADBECAJghQgAAM0QIAGCGCAEAzBAhAIAZIgQAMJPxH2qH3JWLi09icrHYJ7gTAgCYIUIAADNECABghggBAMwQIQCAGSIEADBDhAAAZogQAMAMEQIAmCFCAAAzRAgAYIYIAQDMECEAgBlW0QaQFqyIjVRwJwQAMEOEAABmiBAAwAwRAgCYIUIAADNECABghggBAMwQIQCAGSIEADBDhAAAZogQAMAMEQIAmGEBU6QslQUrz965lvY5stFUX+yTvydMFu6EAABmiBAAwAwRAgCYIUIAADNECABghggBAMwQIQCAGSIEADBDhAAAZogQAMAMEQIAmCFCAAAzLGCKSTVZC3eyAOezmeoLrCJ3cCcEADBDhAAAZiYcoQsXLmjDhg0KhULyeDw6ceJE0vPOOTU1NSkUCqmwsFDV1dW6fv16uuYFAOSQCUdoaGhICxcuVGtr65jP79mzRy0tLWptbVVnZ6cCgYDWrVunwcHBZx4WAJBbJvzBhHA4rHA4POZzzjnt27dPO3fuVG1trSTp4MGDKi0t1ZEjR/T6668/27QAgJyS1veEenp6FIlEVFNTk9jn9Xq1evVqXbp0acyvicfjisViSRsAID+kNUKRSESSVFpamrS/tLQ08dyTmpub5ff7E1tZWVk6RwIATGEZ+XScx+NJeuycG7XvsR07digajSa23t7eTIwEAJiC0vrNqoFAQNLIHVEwGEzs7+/vH3V39JjX65XX603nGACALJHWO6GKigoFAgG1tbUl9j148EAdHR2qqqpK50sBAHLAhO+E7t27p88++yzxuKenR9euXVNxcbFeeOEFNTQ0aPfu3Zo7d67mzp2r3bt367nnntNrr72W1sEBANlvwhH6+OOPtWbNmsTjxsZGSVJdXZ3+/Oc/65133tH9+/f15ptv6vPPP9fSpUv10UcfyefzpW9qAEBO8DjnnPUQXxeLxeT3+1WtjZrhKbAeB1lqqi9gygKhyGXD7qHadVLRaFRFRUXjHsvacQAAM0QIAGCGCAEAzBAhAIAZIgQAMEOEAABmiBAAwAwRAgCYIUIAADNECABghggBAMwQIQCAGSIEADCT1p+sCkwVqa5SPdVX3wZyDXdCAAAzRAgAYIYIAQDMECEAgBkiBAAwQ4QAAGaIEADADBECAJghQgAAM0QIAGCGCAEAzBAhAIAZFjAFviaVhU9TWfR0qi+UmuoCsMBEcScEADBDhAAAZogQAMAMEQIAmCFCAAAzRAgAYIYIAQDMECEAgBkiBAAwQ4QAAGaIEADADBECAJhhAVPgGU3WoqeTKZX5WPQUqeBOCABghggBAMwQIQCAGSIEADBDhAAAZogQAMAMEQIAmCFCAAAzRAgAYIYIAQDMECEAgBkiBAAwwwKmgAEWPR3BoqfgTggAYIYIAQDMTDhCFy5c0IYNGxQKheTxeHTixImk57ds2SKPx5O0LVu2LF3zAgByyIQjNDQ0pIULF6q1tfUbj1m/fr36+voS25kzZ55pSABAbprwBxPC4bDC4fC4x3i9XgUCgZSHAgDkh4y8J9Te3q6SkhLNmzdPW7duVX9//zceG4/HFYvFkjYAQH5Ie4TC4bAOHz6sc+fOae/evers7NTatWsVj8fHPL65uVl+vz+xlZWVpXskAMAUlfbvE9q8eXPi15WVlVq8eLHKy8t1+vRp1dbWjjp+x44damxsTDyOxWKECADyRMa/WTUYDKq8vFzd3d1jPu/1euX1ejM9BgBgCsr49wkNDAyot7dXwWAw0y8FAMgyE74Tunfvnj777LPE456eHl27dk3FxcUqLi5WU1OTXn75ZQWDQd26dUu/+c1vNHv2bL300ktpHRwAkP0mHKGPP/5Ya9asSTx+/H5OXV2d9u/fr66uLh06dEhffPGFgsGg1qxZo2PHjsnn86VvagBATvA455z1EF8Xi8Xk9/tVrY2a4SmwHgfIalN90dNUsOjp1DfsHqpdJxWNRlVUVDTusawdBwAwQ4QAAGaIEADADBECAJghQgAAM0QIAGCGCAEAzBAhAIAZIgQAMEOEAABmiBAAwAwRAgCYIUIAADMZ/8mqAOykuuJ0Lq6+jamJOyEAgBkiBAAwQ4QAAGaIEADADBECAJghQgAAM0QIAGCGCAEAzBAhAIAZIgQAMEOEAABmiBAAwAwLmAJZgkVFkYu4EwIAmCFCAAAzRAgAYIYIAQDMECEAgBkiBAAwQ4QAAGaIEADADBECAJghQgAAM0QIAGCGCAEAzLCAKfCMWFgUSB13QgAAM0QIAGCGCAEAzBAhAIAZIgQAMEOEAABmiBAAwAwRAgCYIUIAADNECABghggBAMwQIQCAGRYwRU5iUdHJ95PQD6xHQBbiTggAYIYIAQDMTChCzc3NWrJkiXw+n0pKSrRp0ybduHEj6RjnnJqamhQKhVRYWKjq6mpdv349rUMDAHLDhCLU0dGh+vp6Xb58WW1tbRoeHlZNTY2GhoYSx+zZs0ctLS1qbW1VZ2enAoGA1q1bp8HBwbQPDwDIbhP6YMKHH36Y9PjAgQMqKSnRlStXtGrVKjnntG/fPu3cuVO1tbWSpIMHD6q0tFRHjhzR66+/nr7JAQBZ75neE4pGo5Kk4uJiSVJPT48ikYhqamoSx3i9Xq1evVqXLl0a8/eIx+OKxWJJGwAgP6QcIeecGhsbtWLFClVWVkqSIpGIJKm0tDTp2NLS0sRzT2pubpbf709sZWVlqY4EAMgyKUdo27Zt+uSTT/TXv/511HMejyfpsXNu1L7HduzYoWg0mth6e3tTHQkAkGVS+mbV7du369SpU7pw4YLmzJmT2B8IBCSN3BEFg8HE/v7+/lF3R495vV55vd5UxgAAZLkJ3Qk557Rt2zYdP35c586dU0VFRdLzFRUVCgQCamtrS+x78OCBOjo6VFVVlZ6JAQA5Y0J3QvX19Tpy5IhOnjwpn8+XeJ/H7/ersLBQHo9HDQ0N2r17t+bOnau5c+dq9+7deu655/Taa69l5A8AAMheE4rQ/v37JUnV1dVJ+w8cOKAtW7ZIkt555x3dv39fb775pj7//HMtXbpUH330kXw+X1oGBgDkDo9zzlkP8XWxWEx+v1/V2qgZngLrcTAFsBhp6lhUFBaG3UO166Si0aiKiorGPZa14wAAZogQAMAMEQIAmCFCAAAzRAgAYIYIAQDMECEAgBkiBAAwQ4QAAGaIEADADBECAJghQgAAM0QIAGAmpZ+sitzCKtWTj9WtgRHcCQEAzBAhAIAZIgQAMEOEAABmiBAAwAwRAgCYIUIAADNECABghggBAMwQIQCAGSIEADBDhAAAZljAdApjYdHswGKkQOq4EwIAmCFCAAAzRAgAYIYIAQDMECEAgBkiBAAwQ4QAAGaIEADADBECAJghQgAAM0QIAGCGCAEAzLCA6SRhMdLJxaKiQHbgTggAYIYIAQDMECEAgBkiBAAwQ4QAAGaIEADADBECAJghQgAAM0QIAGCGCAEAzBAhAIAZIgQAMMMCppOEBTUBYDTuhAAAZogQAMDMhCLU3NysJUuWyOfzqaSkRJs2bdKNGzeSjtmyZYs8Hk/StmzZsrQODQDIDROKUEdHh+rr63X58mW1tbVpeHhYNTU1GhoaSjpu/fr16uvrS2xnzpxJ69AAgNwwoQ8mfPjhh0mPDxw4oJKSEl25ckWrVq1K7Pd6vQoEAumZEACQs57pPaFoNCpJKi4uTtrf3t6ukpISzZs3T1u3blV/f/83/h7xeFyxWCxpAwDkh5Qj5JxTY2OjVqxYocrKysT+cDisw4cP69y5c9q7d686Ozu1du1axePxMX+f5uZm+f3+xFZWVpbqSACALONxzrlUvrC+vl6nT5/WxYsXNWfOnG88rq+vT+Xl5Tp69Khqa2tHPR+Px5MCFYvFVFZWpmpt1AxPQSqjAQAMDbuHatdJRaNRFRUVjXtsSt+sun37dp06dUoXLlwYN0CSFAwGVV5eru7u7jGf93q98nq9qYwBAMhyE4qQc07bt2/XBx98oPb2dlVUVDz1awYGBtTb26tgMJjykACA3DSh94Tq6+v1l7/8RUeOHJHP51MkElEkEtH9+/clSffu3dPbb7+tf/zjH7p165ba29u1YcMGzZ49Wy+99FJG/gAAgOw1oTuh/fv3S5Kqq6uT9h84cEBbtmzR9OnT1dXVpUOHDumLL75QMBjUmjVrdOzYMfl8vrQNDQDIDRP+57jxFBYW6uzZs880EAAgf7B2HADADBECAJghQgAAM0QIAGCGCAEAzBAhAIAZIgQAMEOEAABmiBAAwAwRAgCYIUIAADNECABghggBAMwQIQCAGSIEADBDhAAAZogQAMAMEQIAmCFCAAAzRAgAYIYIAQDMECEAgBkiBAAwQ4QAAGaIEADAzAzrAZ7knJMkDeuh5IyHAQBM2LAeSvr//56PZ8pFaHBwUJJ0UWeMJwEAPIvBwUH5/f5xj/G4b5OqSfTo0SPduXNHPp9PHo8n6blYLKaysjL19vaqqKjIaEJ7nIcRnIcRnIcRnIcRU+E8OOc0ODioUCikadPGf9dnyt0JTZs2TXPmzBn3mKKiory+yB7jPIzgPIzgPIzgPIywPg9PuwN6jA8mAADMECEAgJmsipDX69WuXbvk9XqtRzHFeRjBeRjBeRjBeRiRbedhyn0wAQCQP7LqTggAkFuIEADADBECAJghQgAAM1kVoffee08VFRX6zne+o0WLFunvf/+79UiTqqmpSR6PJ2kLBALWY2XchQsXtGHDBoVCIXk8Hp04cSLpeeecmpqaFAqFVFhYqOrqal2/ft1m2Ax62nnYsmXLqOtj2bJlNsNmSHNzs5YsWSKfz6eSkhJt2rRJN27cSDomH66Hb3MesuV6yJoIHTt2TA0NDdq5c6euXr2qlStXKhwO6/bt29ajTar58+err68vsXV1dVmPlHFDQ0NauHChWltbx3x+z549amlpUWtrqzo7OxUIBLRu3brEOoS54mnnQZLWr1+fdH2cOZNbazB2dHSovr5ely9fVltbm4aHh1VTU6OhoaHEMflwPXyb8yBlyfXgssSPfvQj98YbbyTte/HFF92vf/1ro4km365du9zChQutxzAlyX3wwQeJx48ePXKBQMC9++67iX3//e9/nd/vd3/4wx8MJpwcT54H55yrq6tzGzduNJnHSn9/v5PkOjo6nHP5ez08eR6cy57rISvuhB48eKArV66opqYmaX9NTY0uXbpkNJWN7u5uhUIhVVRU6JVXXtHNmzetRzLV09OjSCSSdG14vV6tXr06764NSWpvb1dJSYnmzZunrVu3qr+/33qkjIpGo5Kk4uJiSfl7PTx5Hh7LhushKyJ09+5dffXVVyotLU3aX1paqkgkYjTV5Fu6dKkOHTqks2fP6v3331ckElFVVZUGBgasRzPz+O8/368NSQqHwzp8+LDOnTunvXv3qrOzU2vXrlU8HrceLSOcc2psbNSKFStUWVkpKT+vh7HOg5Q918OUW0V7PE/+aAfn3Kh9uSwcDid+vWDBAi1fvlzf+973dPDgQTU2NhpOZi/frw1J2rx5c+LXlZWVWrx4scrLy3X69GnV1tYaTpYZ27Zt0yeffKKLFy+Oei6frodvOg/Zcj1kxZ3Q7NmzNX369FH/J9Pf3z/q/3jyyaxZs7RgwQJ1d3dbj2Lm8acDuTZGCwaDKi8vz8nrY/v27Tp16pTOnz+f9KNf8u16+KbzMJapej1kRYRmzpypRYsWqa2tLWl/W1ubqqqqjKayF4/H9emnnyoYDFqPYqaiokKBQCDp2njw4IE6Ojry+tqQpIGBAfX29ubU9eGc07Zt23T8+HGdO3dOFRUVSc/ny/XwtPMwlil7PRh+KGJCjh496goKCtyf/vQn969//cs1NDS4WbNmuVu3blmPNmneeust197e7m7evOkuX77sfvrTnzqfz5fz52BwcNBdvXrVXb161UlyLS0t7urVq+7f//63c865d9991/n9fnf8+HHX1dXlXn31VRcMBl0sFjOePL3GOw+Dg4PurbfecpcuXXI9PT3u/Pnzbvny5e673/1uTp2HX/3qV87v97v29nbX19eX2L788svEMflwPTztPGTT9ZA1EXLOud///veuvLzczZw50/3whz9M+jhiPti8ebMLBoOuoKDAhUIhV1tb665fv249VsadP3/eSRq11dXVOedGPpa7a9cuFwgEnNfrdatWrXJdXV22Q2fAeOfhyy+/dDU1Ne755593BQUF7oUXXnB1dXXu9u3b1mOn1Vh/fknuwIEDiWPy4Xp42nnIpuuBH+UAADCTFe8JAQByExECAJghQgAAM0QIAGCGCAEAzBAhAIAZIgQAMEOEAABmiBAAwAwRAgCYIUIAADNECABg5n8BDUpUpaiWHfsAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img = next(iter(dst_train))\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dst_train_rdy = dst_train.batch(64)\n",
    "dst_val_rdy = dst_val.batch(64)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model blocks\n",
    "\n",
    "One of the key components of the model is their masking. They do this so that the model doesn't see the future pixels to predict a specific pixel.\n",
    "\n",
    "The idea is to generate a mask like the following:"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They have a different kind of mask where the central pixel is also a 1. so that it can be connected to itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PixelConv(nn.Module):\n",
    "    \"\"\"Masked convolution as in PixelCNN.\"\"\"\n",
    "    mask_type: str\n",
    "    features: int\n",
    "    kernel_size: Sequence[int]\n",
    "    strides: int = 1\n",
    "    padding: str = \"SAME\"\n",
    "    use_bias: bool = True\n",
    "    kernel_init: Callable = nn.initializers.lecun_normal()\n",
    "    bias_init: Callable = nn.initializers.zeros_init()\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, inputs):\n",
    "        ## 0. Instantiate the kernel and bias of the convolution\n",
    "        kernel = self.param(\"kernel\",\n",
    "                            self.kernel_init,\n",
    "                            (self.kernel_size[0], self.kernel_size[1], inputs.shape[-1], self.features))\n",
    "        bias = self.param(\"bias\",\n",
    "                          self.bias_init,\n",
    "                          (self.features,)) if self.use_bias else 0.\n",
    "\n",
    "        ## 1. Build the mask\n",
    "        mask = self.generate_mask(kernel.shape, self.mask_type)\n",
    "\n",
    "        ## 2. Calculate the masked convolution\n",
    "        ## Add the batch dim if the input is a single element\n",
    "        if jnp.ndim(inputs) < 4: inputs = inputs[None,:]; had_batch = False\n",
    "        else: had_batch = True\n",
    "        outputs = lax.conv(jnp.transpose(inputs,[0,3,1,2]),    # lhs = NCHW image tensor\n",
    "               jnp.transpose(kernel*mask,[3,2,0,1]), # rhs = OIHW conv kernel tensor\n",
    "               (self.strides, self.strides),\n",
    "               self.padding)\n",
    "        ## Move the channels back to the last dim\n",
    "        outputs = jnp.transpose(outputs, (0,2,3,1))\n",
    "        if not had_batch: outputs = outputs[0]\n",
    "        return outputs + bias\n",
    "    \n",
    "    @staticmethod\n",
    "    def generate_mask(kernel_size, mask_type):\n",
    "        mask = jnp.zeros(shape=kernel_size)\n",
    "        mask = mask.at[:kernel_size[0]//2].set(1.)\n",
    "        mask = mask.at[kernel_size[0]//2,:kernel_size[1]//2].set(1.)\n",
    "        if mask_type == \"B\":\n",
    "            mask = mask.at[kernel_size[0]//2,kernel_size[1]//2].set(1.)\n",
    "        return mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can initialize the parameters of the layer to see if everything works as intended:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FrozenDict({\n",
       "    params: {\n",
       "        bias: (1,),\n",
       "        kernel: (5, 5, 1, 1),\n",
       "    },\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = PixelConv(mask_type=\"B\", features=1, kernel_size=(5,5))\n",
    "key = random.PRNGKey(0)\n",
    "params = l.init(key, jnp.ones((1,28,28,1)))\n",
    "jax.tree_map(lambda x: x.shape, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And as a sanity check we can as well check that the `.generate_mask()` method behaves as intended for both cases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1.],\n",
       "       [1., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = PixelConv(mask_type=\"A\", features=1, kernel_size=(5,5))\n",
    "l.generate_mask(kernel_size=(5,5), mask_type=\"A\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = PixelConv(mask_type=\"B\", features=1, kernel_size=(5,5))\n",
    "l.generate_mask(kernel_size=(5,5), mask_type=\"B\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They also define a residual block based on this masked convolution, so we'll follow along:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"Residual block based on the `PixelConv` layer.\"\"\"\n",
    "    mask_type: str\n",
    "    features: int\n",
    "    kernel_size: Sequence[int]\n",
    "    strides: int = 1\n",
    "    padding: str = \"SAME\"\n",
    "    use_bias: bool = True\n",
    "    kernel_init: Callable = nn.initializers.lecun_normal()\n",
    "    bias_init: Callable = nn.initializers.zeros_init()\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, inputs):\n",
    "        outputs = nn.Conv(self.features, kernel_size=(1,1))(inputs)\n",
    "        outputs = nn.relu(outputs)\n",
    "        outputs = PixelConv(features=self.features//2, kernel_size=self.kernel_size, mask_type=self.mask_type)(outputs)\n",
    "        outputs = nn.relu(outputs)\n",
    "        outputs = nn.Conv(self.features, kernel_size=(1,1))(outputs)\n",
    "        outputs = nn.relu(outputs)\n",
    "        return inputs + outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FrozenDict({\n",
       "    params: {\n",
       "        Conv_0: {\n",
       "            bias: (2,),\n",
       "            kernel: (1, 1, 1, 2),\n",
       "        },\n",
       "        Conv_1: {\n",
       "            bias: (2,),\n",
       "            kernel: (1, 1, 1, 2),\n",
       "        },\n",
       "        PixelConv_0: {\n",
       "            bias: (1,),\n",
       "            kernel: (5, 5, 2, 1),\n",
       "        },\n",
       "    },\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = ResidualBlock(mask_type=\"B\", features=2, kernel_size=(5,5))\n",
    "key = random.PRNGKey(0)\n",
    "params = l.init(key, jnp.ones((1,28,28,1)))\n",
    "jax.tree_map(lambda x: x.shape, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the original model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PixelCNN(nn.Module):\n",
    "    \"\"\"PixelCNN model as stated in their original paper (or so they said in the Keras example).\"\"\"\n",
    "\n",
    "    n_residual_blocks: int = 5\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, inputs):\n",
    "        outputs = PixelConv(features=128, kernel_size=(7,7), mask_type=\"A\")(inputs)\n",
    "        outputs = nn.relu(outputs)\n",
    "        for _ in range(self.n_residual_blocks):\n",
    "            outputs = ResidualBlock(features=128, kernel_size=(3,3), mask_type=\"B\")(outputs)\n",
    "            outputs = nn.relu(outputs)\n",
    "        outputs = PixelConv(features=128, kernel_size=(1,1), mask_type=\"B\")(outputs)\n",
    "        outputs = nn.relu(outputs)\n",
    "        outputs = PixelConv(features=128, kernel_size=(1,1), mask_type=\"B\")(outputs)\n",
    "        outputs = nn.relu(outputs)\n",
    "        outputs = nn.Conv(features=1, kernel_size=(1,1))(outputs)\n",
    "        ## Skip the sigmoid activation because it's included in the loss function we will be using\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparations before training\n",
    "\n",
    "> Things like building the `TrainState` and so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "@struct.dataclass\n",
    "class Metrics(metrics.Collection):\n",
    "    \"\"\"Collection of metrics to be tracked during training.\"\"\"\n",
    "    loss: metrics.Average.from_output(\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainState(train_state.TrainState):\n",
    "    metrics: Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_state(module, key, tx, input_shape):\n",
    "    \"\"\"Creates the initial `TrainState`.\"\"\"\n",
    "    params = module.init(key, jnp.ones(input_shape))[\"params\"]\n",
    "    return TrainState.create(\n",
    "        apply_fn=module.apply,\n",
    "        params=params,\n",
    "        tx=tx,\n",
    "        metrics=Metrics.empty()\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def train_step(state, batch):\n",
    "    \"\"\"Train for a single step.\"\"\"\n",
    "    inputs = batch\n",
    "    def loss_fn(params):\n",
    "        pred = state.apply_fn({\"params\": params}, inputs)\n",
    "        loss = optax.sigmoid_binary_cross_entropy(logits=pred, labels=inputs).mean()\n",
    "        return loss, pred\n",
    "    (loss, pred), grads = jax.value_and_grad(loss_fn, has_aux=True)(state.params)\n",
    "    state = state.apply_gradients(grads=grads)\n",
    "    def compute_metrics(state):\n",
    "        metric_updates = state.metrics.single_from_model_output(\n",
    "            logits=pred, labels=inputs, loss=loss,\n",
    "        )\n",
    "        metrics = state.metrics.merge(metric_updates)\n",
    "        state = state.replace(metrics=metrics)\n",
    "        return state\n",
    "    state = compute_metrics(state)\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def compute_metrics(*, state, batch):\n",
    "    \"\"\"Obtaining the metrics for a given batch.\"\"\"\n",
    "    inputs = batch\n",
    "    pred = state.apply_fn({\"params\": state.params}, inputs)\n",
    "    loss = optax.sigmoid_binary_cross_entropy(logits=pred, labels=inputs).mean()\n",
    "    metric_updates = state.metrics.single_from_model_output(\n",
    "        logits=pred, labels=inputs, loss=loss,\n",
    "    )\n",
    "    metrics = state.metrics.merge(metric_updates)\n",
    "    state = state.replace(metrics=metrics)\n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = create_train_state(PixelCNN(), random.PRNGKey(0), optax.adam(3e-4), input_shape=(1,28,28,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orbax_checkpointer = orbax.checkpoint.PyTreeCheckpointer()\n",
    "save_args = orbax_utils.save_args_from_target(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_history = {\n",
    "    \"train_loss\": [],\n",
    "    \"val_loss\": [],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 -> [Train] Loss: 0.10738322883844376 | [Val] Loss: 0.0964701697230339\n",
      "Epoch 1 -> [Train] Loss: 0.09107647091150284 | [Val] Loss: 0.09731429815292358\n",
      "Epoch 2 -> [Train] Loss: 0.08957286179065704 | [Val] Loss: 0.10225003212690353\n",
      "Epoch 3 -> [Train] Loss: 0.08864208310842514 | [Val] Loss: 0.09415137022733688\n",
      "Epoch 4 -> [Train] Loss: 0.08789239078760147 | [Val] Loss: 0.09250602126121521\n",
      "Epoch 5 -> [Train] Loss: 0.08720973879098892 | [Val] Loss: 0.09165811538696289\n",
      "Epoch 6 -> [Train] Loss: 0.08665670454502106 | [Val] Loss: 0.08975152671337128\n",
      "Epoch 7 -> [Train] Loss: 0.08617578446865082 | [Val] Loss: 0.08826755732297897\n",
      "Epoch 8 -> [Train] Loss: 0.0857725515961647 | [Val] Loss: 0.08740673214197159\n",
      "Epoch 9 -> [Train] Loss: 0.08542186766862869 | [Val] Loss: 0.08710061013698578\n",
      "Epoch 10 -> [Train] Loss: 0.0851156935095787 | [Val] Loss: 0.08661967515945435\n",
      "Epoch 11 -> [Train] Loss: 0.08483607321977615 | [Val] Loss: 0.0863041803240776\n",
      "Epoch 12 -> [Train] Loss: 0.08458948880434036 | [Val] Loss: 0.08611737936735153\n",
      "Epoch 13 -> [Train] Loss: 0.08436410874128342 | [Val] Loss: 0.08581117540597916\n",
      "Epoch 14 -> [Train] Loss: 0.08416550606489182 | [Val] Loss: 0.08576489239931107\n",
      "Epoch 15 -> [Train] Loss: 0.08398126810789108 | [Val] Loss: 0.08567188680171967\n",
      "Epoch 16 -> [Train] Loss: 0.083806112408638 | [Val] Loss: 0.08569351583719254\n",
      "Epoch 17 -> [Train] Loss: 0.08364490419626236 | [Val] Loss: 0.0856267660856247\n",
      "Epoch 18 -> [Train] Loss: 0.08348613232374191 | [Val] Loss: 0.08566184341907501\n",
      "Epoch 19 -> [Train] Loss: 0.08333931118249893 | [Val] Loss: 0.08561475574970245\n",
      "Epoch 20 -> [Train] Loss: 0.08320430666208267 | [Val] Loss: 0.08549924939870834\n",
      "Epoch 21 -> [Train] Loss: 0.08308349549770355 | [Val] Loss: 0.08538253605365753\n",
      "Epoch 22 -> [Train] Loss: 0.08296020328998566 | [Val] Loss: 0.08526483923196793\n",
      "Epoch 23 -> [Train] Loss: 0.08284399658441544 | [Val] Loss: 0.08516667038202286\n",
      "Epoch 24 -> [Train] Loss: 0.08273840695619583 | [Val] Loss: 0.08519885689020157\n",
      "Epoch 25 -> [Train] Loss: 0.08264273405075073 | [Val] Loss: 0.08530835062265396\n",
      "Epoch 26 -> [Train] Loss: 0.08255711942911148 | [Val] Loss: 0.08537391573190689\n",
      "Epoch 27 -> [Train] Loss: 0.08246186375617981 | [Val] Loss: 0.08547496050596237\n",
      "Epoch 28 -> [Train] Loss: 0.08237452805042267 | [Val] Loss: 0.0855708196759224\n",
      "Epoch 29 -> [Train] Loss: 0.08228433132171631 | [Val] Loss: 0.08544091880321503\n",
      "Epoch 30 -> [Train] Loss: 0.08220058679580688 | [Val] Loss: 0.08529998362064362\n",
      "Epoch 31 -> [Train] Loss: 0.08211592584848404 | [Val] Loss: 0.08522374927997589\n",
      "Epoch 32 -> [Train] Loss: 0.0820421501994133 | [Val] Loss: 0.08519421517848969\n",
      "Epoch 33 -> [Train] Loss: 0.0819670632481575 | [Val] Loss: 0.08524831384420395\n",
      "Epoch 34 -> [Train] Loss: 0.08190534263849258 | [Val] Loss: 0.0851941779255867\n",
      "Epoch 35 -> [Train] Loss: 0.08182869851589203 | [Val] Loss: 0.08521327376365662\n",
      "Epoch 36 -> [Train] Loss: 0.08175637573003769 | [Val] Loss: 0.08516815304756165\n",
      "Epoch 37 -> [Train] Loss: 0.0816769227385521 | [Val] Loss: 0.08506952226161957\n",
      "Epoch 38 -> [Train] Loss: 0.08159826695919037 | [Val] Loss: 0.08512453734874725\n",
      "Epoch 39 -> [Train] Loss: 0.08153078705072403 | [Val] Loss: 0.08520075678825378\n",
      "Epoch 40 -> [Train] Loss: 0.08146360516548157 | [Val] Loss: 0.08520692586898804\n",
      "Epoch 41 -> [Train] Loss: 0.08140581846237183 | [Val] Loss: 0.08523555845022202\n",
      "Epoch 42 -> [Train] Loss: 0.08134026825428009 | [Val] Loss: 0.0852491557598114\n",
      "Epoch 43 -> [Train] Loss: 0.08129159361124039 | [Val] Loss: 0.0852651447057724\n",
      "Epoch 44 -> [Train] Loss: 0.08124976605176926 | [Val] Loss: 0.08525235205888748\n",
      "Epoch 45 -> [Train] Loss: 0.08118940144777298 | [Val] Loss: 0.08525300025939941\n",
      "Epoch 46 -> [Train] Loss: 0.08113618940114975 | [Val] Loss: 0.08513954281806946\n",
      "Epoch 47 -> [Train] Loss: 0.08106967061758041 | [Val] Loss: 0.08529540151357651\n",
      "Epoch 48 -> [Train] Loss: 0.0810319036245346 | [Val] Loss: 0.08528003841638565\n",
      "Epoch 49 -> [Train] Loss: 0.08097992092370987 | [Val] Loss: 0.08525583148002625\n",
      "Epoch 50 -> [Train] Loss: 0.08092436194419861 | [Val] Loss: 0.08527357131242752\n",
      "Epoch 51 -> [Train] Loss: 0.08086798340082169 | [Val] Loss: 0.08520007878541946\n",
      "Epoch 52 -> [Train] Loss: 0.08082878589630127 | [Val] Loss: 0.08521276712417603\n",
      "Epoch 53 -> [Train] Loss: 0.08076874166727066 | [Val] Loss: 0.08530198782682419\n",
      "Epoch 54 -> [Train] Loss: 0.0807277262210846 | [Val] Loss: 0.08527864515781403\n",
      "Epoch 55 -> [Train] Loss: 0.08066593110561371 | [Val] Loss: 0.08527880162000656\n",
      "Epoch 56 -> [Train] Loss: 0.0806330069899559 | [Val] Loss: 0.08547786623239517\n",
      "Epoch 57 -> [Train] Loss: 0.08059027791023254 | [Val] Loss: 0.08563311398029327\n",
      "Epoch 58 -> [Train] Loss: 0.08053097873926163 | [Val] Loss: 0.0857529491186142\n",
      "Epoch 59 -> [Train] Loss: 0.0804889053106308 | [Val] Loss: 0.08573626726865768\n",
      "Epoch 60 -> [Train] Loss: 0.08044218271970749 | [Val] Loss: 0.08581312000751495\n",
      "Epoch 61 -> [Train] Loss: 0.08040136843919754 | [Val] Loss: 0.08583568036556244\n",
      "Epoch 62 -> [Train] Loss: 0.08036261796951294 | [Val] Loss: 0.08602148294448853\n",
      "Epoch 63 -> [Train] Loss: 0.08033232390880585 | [Val] Loss: 0.08602829277515411\n",
      "Epoch 64 -> [Train] Loss: 0.0802973136305809 | [Val] Loss: 0.08596452325582504\n",
      "Epoch 65 -> [Train] Loss: 0.0802403911948204 | [Val] Loss: 0.08610252290964127\n",
      "Epoch 66 -> [Train] Loss: 0.08019731938838959 | [Val] Loss: 0.08622755855321884\n",
      "Epoch 67 -> [Train] Loss: 0.08015749603509903 | [Val] Loss: 0.08622650057077408\n",
      "Epoch 68 -> [Train] Loss: 0.08011564612388611 | [Val] Loss: 0.08625362068414688\n",
      "Epoch 69 -> [Train] Loss: 0.08009455353021622 | [Val] Loss: 0.08617907762527466\n",
      "Epoch 70 -> [Train] Loss: 0.08007935434579849 | [Val] Loss: 0.08634526282548904\n",
      "Epoch 71 -> [Train] Loss: 0.08004669100046158 | [Val] Loss: 0.08638672530651093\n",
      "Epoch 72 -> [Train] Loss: 0.08001033961772919 | [Val] Loss: 0.08638772368431091\n",
      "Epoch 73 -> [Train] Loss: 0.07998045533895493 | [Val] Loss: 0.08643532544374466\n",
      "Epoch 74 -> [Train] Loss: 0.0799410343170166 | [Val] Loss: 0.08664171397686005\n",
      "Epoch 75 -> [Train] Loss: 0.07990028709173203 | [Val] Loss: 0.08673126995563507\n",
      "Epoch 76 -> [Train] Loss: 0.07986626774072647 | [Val] Loss: 0.08717221021652222\n",
      "Epoch 77 -> [Train] Loss: 0.07982156425714493 | [Val] Loss: 0.08737879246473312\n",
      "Epoch 78 -> [Train] Loss: 0.07979520410299301 | [Val] Loss: 0.08751213550567627\n",
      "Epoch 79 -> [Train] Loss: 0.07976660132408142 | [Val] Loss: 0.087653748691082\n",
      "Epoch 80 -> [Train] Loss: 0.07973561435937881 | [Val] Loss: 0.08776026964187622\n",
      "Epoch 81 -> [Train] Loss: 0.07970365881919861 | [Val] Loss: 0.08754584938287735\n",
      "Epoch 82 -> [Train] Loss: 0.07965406775474548 | [Val] Loss: 0.08758016675710678\n",
      "Epoch 83 -> [Train] Loss: 0.07962775975465775 | [Val] Loss: 0.0874653086066246\n",
      "Epoch 84 -> [Train] Loss: 0.0795874297618866 | [Val] Loss: 0.08741036057472229\n",
      "Epoch 85 -> [Train] Loss: 0.07955688238143921 | [Val] Loss: 0.08748070150613785\n",
      "Epoch 86 -> [Train] Loss: 0.07952617853879929 | [Val] Loss: 0.087435282766819\n",
      "Epoch 87 -> [Train] Loss: 0.07950039207935333 | [Val] Loss: 0.08745390921831131\n",
      "Epoch 88 -> [Train] Loss: 0.07946451008319855 | [Val] Loss: 0.0875827819108963\n",
      "Epoch 89 -> [Train] Loss: 0.07942890375852585 | [Val] Loss: 0.08760502189397812\n",
      "Epoch 90 -> [Train] Loss: 0.07941056787967682 | [Val] Loss: 0.08778133988380432\n",
      "Epoch 91 -> [Train] Loss: 0.07939723134040833 | [Val] Loss: 0.0875898003578186\n",
      "Epoch 92 -> [Train] Loss: 0.07936208695173264 | [Val] Loss: 0.087405264377594\n",
      "Epoch 93 -> [Train] Loss: 0.0793636366724968 | [Val] Loss: 0.08763737976551056\n",
      "Epoch 94 -> [Train] Loss: 0.07934217154979706 | [Val] Loss: 0.08781559765338898\n",
      "Epoch 95 -> [Train] Loss: 0.07931946218013763 | [Val] Loss: 0.08762340992689133\n",
      "Epoch 96 -> [Train] Loss: 0.07928448915481567 | [Val] Loss: 0.08772603422403336\n",
      "Epoch 97 -> [Train] Loss: 0.07927937805652618 | [Val] Loss: 0.08774618059396744\n",
      "Epoch 98 -> [Train] Loss: 0.07924240827560425 | [Val] Loss: 0.08780994266271591\n",
      "Epoch 99 -> [Train] Loss: 0.0792393833398819 | [Val] Loss: 0.08779697865247726\n",
      "Epoch 100 -> [Train] Loss: 0.07920101284980774 | [Val] Loss: 0.08788052201271057\n",
      "Epoch 101 -> [Train] Loss: 0.07914897799491882 | [Val] Loss: 0.08786282688379288\n",
      "Epoch 102 -> [Train] Loss: 0.07913108915090561 | [Val] Loss: 0.0877818912267685\n",
      "Epoch 103 -> [Train] Loss: 0.07911287993192673 | [Val] Loss: 0.0877189114689827\n",
      "Epoch 104 -> [Train] Loss: 0.07908949255943298 | [Val] Loss: 0.08785821497440338\n",
      "Epoch 105 -> [Train] Loss: 0.07905500382184982 | [Val] Loss: 0.08792740851640701\n",
      "Epoch 106 -> [Train] Loss: 0.07904066890478134 | [Val] Loss: 0.08800756931304932\n",
      "Epoch 107 -> [Train] Loss: 0.07901480048894882 | [Val] Loss: 0.08807168155908585\n",
      "Epoch 108 -> [Train] Loss: 0.07897717505693436 | [Val] Loss: 0.08796602487564087\n",
      "Epoch 109 -> [Train] Loss: 0.07896625995635986 | [Val] Loss: 0.0880904495716095\n",
      "Epoch 110 -> [Train] Loss: 0.07894834131002426 | [Val] Loss: 0.08802790194749832\n",
      "Epoch 111 -> [Train] Loss: 0.07893841713666916 | [Val] Loss: 0.08785100281238556\n",
      "Epoch 112 -> [Train] Loss: 0.0789022222161293 | [Val] Loss: 0.08827581256628036\n",
      "Epoch 113 -> [Train] Loss: 0.07889175415039062 | [Val] Loss: 0.08803646266460419\n",
      "Epoch 114 -> [Train] Loss: 0.07887280732393265 | [Val] Loss: 0.08827267587184906\n",
      "Epoch 115 -> [Train] Loss: 0.07887602597475052 | [Val] Loss: 0.08821848034858704\n",
      "Epoch 116 -> [Train] Loss: 0.07883197069168091 | [Val] Loss: 0.08799134194850922\n",
      "Epoch 117 -> [Train] Loss: 0.07885193824768066 | [Val] Loss: 0.0883142501115799\n",
      "Epoch 118 -> [Train] Loss: 0.07881581783294678 | [Val] Loss: 0.08809852600097656\n",
      "Epoch 119 -> [Train] Loss: 0.07882395386695862 | [Val] Loss: 0.08803629875183105\n",
      "Epoch 120 -> [Train] Loss: 0.07878540456295013 | [Val] Loss: 0.08818607777357101\n",
      "Epoch 121 -> [Train] Loss: 0.07876639068126678 | [Val] Loss: 0.08819933980703354\n",
      "Epoch 122 -> [Train] Loss: 0.07875534147024155 | [Val] Loss: 0.0883144736289978\n",
      "Epoch 123 -> [Train] Loss: 0.07873106747865677 | [Val] Loss: 0.08843965828418732\n",
      "Epoch 124 -> [Train] Loss: 0.07872297614812851 | [Val] Loss: 0.08834268897771835\n",
      "Epoch 125 -> [Train] Loss: 0.07870687544345856 | [Val] Loss: 0.08820229023694992\n",
      "Epoch 126 -> [Train] Loss: 0.07870247215032578 | [Val] Loss: 0.08832980692386627\n",
      "Epoch 127 -> [Train] Loss: 0.07866482436656952 | [Val] Loss: 0.08814751356840134\n",
      "Epoch 128 -> [Train] Loss: 0.07861090451478958 | [Val] Loss: 0.08811058849096298\n",
      "Epoch 129 -> [Train] Loss: 0.07863305509090424 | [Val] Loss: 0.08807417750358582\n",
      "Epoch 130 -> [Train] Loss: 0.07860683649778366 | [Val] Loss: 0.08827722817659378\n",
      "Epoch 131 -> [Train] Loss: 0.07858681678771973 | [Val] Loss: 0.08849892020225525\n",
      "Epoch 132 -> [Train] Loss: 0.07855662703514099 | [Val] Loss: 0.08831816911697388\n",
      "Epoch 133 -> [Train] Loss: 0.07853548228740692 | [Val] Loss: 0.0884048119187355\n",
      "Epoch 134 -> [Train] Loss: 0.07850015163421631 | [Val] Loss: 0.0883852168917656\n",
      "Epoch 135 -> [Train] Loss: 0.0785147175192833 | [Val] Loss: 0.08860421925783157\n",
      "Epoch 136 -> [Train] Loss: 0.07851375639438629 | [Val] Loss: 0.08868584036827087\n",
      "Epoch 137 -> [Train] Loss: 0.0784841850399971 | [Val] Loss: 0.0888076052069664\n",
      "Epoch 138 -> [Train] Loss: 0.07845050096511841 | [Val] Loss: 0.08861083537340164\n",
      "Epoch 139 -> [Train] Loss: 0.07843560725450516 | [Val] Loss: 0.0886986032128334\n",
      "Epoch 140 -> [Train] Loss: 0.07843615114688873 | [Val] Loss: 0.0888044461607933\n",
      "Epoch 141 -> [Train] Loss: 0.07841262221336365 | [Val] Loss: 0.0889013484120369\n",
      "Epoch 142 -> [Train] Loss: 0.07843201607465744 | [Val] Loss: 0.08881571888923645\n",
      "Epoch 143 -> [Train] Loss: 0.07837547361850739 | [Val] Loss: 0.08867444097995758\n",
      "Epoch 144 -> [Train] Loss: 0.07836702466011047 | [Val] Loss: 0.08893648535013199\n",
      "Epoch 145 -> [Train] Loss: 0.07835914939641953 | [Val] Loss: 0.08900916576385498\n",
      "Epoch 146 -> [Train] Loss: 0.0783512145280838 | [Val] Loss: 0.08890786021947861\n",
      "Epoch 147 -> [Train] Loss: 0.07835422456264496 | [Val] Loss: 0.08907727897167206\n",
      "Epoch 148 -> [Train] Loss: 0.07831881195306778 | [Val] Loss: 0.08913495391607285\n",
      "Epoch 149 -> [Train] Loss: 0.07834481447935104 | [Val] Loss: 0.0890706330537796\n",
      "Epoch 150 -> [Train] Loss: 0.07829877734184265 | [Val] Loss: 0.08889220654964447\n",
      "Epoch 151 -> [Train] Loss: 0.07828589528799057 | [Val] Loss: 0.08896445482969284\n",
      "Epoch 152 -> [Train] Loss: 0.07830256968736649 | [Val] Loss: 0.08885406702756882\n",
      "Epoch 153 -> [Train] Loss: 0.07829461991786957 | [Val] Loss: 0.08904972672462463\n",
      "Epoch 154 -> [Train] Loss: 0.07824879884719849 | [Val] Loss: 0.08897685259580612\n",
      "Epoch 155 -> [Train] Loss: 0.07823782414197922 | [Val] Loss: 0.08898639678955078\n",
      "Epoch 156 -> [Train] Loss: 0.07821251451969147 | [Val] Loss: 0.0892292857170105\n",
      "Epoch 157 -> [Train] Loss: 0.07819056510925293 | [Val] Loss: 0.08896613121032715\n",
      "Epoch 158 -> [Train] Loss: 0.07818156480789185 | [Val] Loss: 0.08906383067369461\n",
      "Epoch 159 -> [Train] Loss: 0.07813629508018494 | [Val] Loss: 0.08907748013734818\n",
      "Epoch 160 -> [Train] Loss: 0.07813147455453873 | [Val] Loss: 0.0887974426150322\n",
      "Epoch 161 -> [Train] Loss: 0.07810575515031815 | [Val] Loss: 0.08917000889778137\n",
      "Epoch 162 -> [Train] Loss: 0.07810043543577194 | [Val] Loss: 0.08909148722887039\n",
      "Epoch 163 -> [Train] Loss: 0.07806464284658432 | [Val] Loss: 0.08905136585235596\n",
      "Epoch 164 -> [Train] Loss: 0.07804068922996521 | [Val] Loss: 0.08919773250818253\n",
      "Epoch 165 -> [Train] Loss: 0.07800757884979248 | [Val] Loss: 0.08922033756971359\n",
      "Epoch 166 -> [Train] Loss: 0.07804147154092789 | [Val] Loss: 0.08913084119558334\n",
      "Epoch 167 -> [Train] Loss: 0.0780215784907341 | [Val] Loss: 0.08928553760051727\n",
      "Epoch 168 -> [Train] Loss: 0.07802242785692215 | [Val] Loss: 0.08922761678695679\n",
      "Epoch 169 -> [Train] Loss: 0.07800431549549103 | [Val] Loss: 0.08919224888086319\n",
      "Epoch 170 -> [Train] Loss: 0.07796520739793777 | [Val] Loss: 0.08922488242387772\n",
      "Epoch 171 -> [Train] Loss: 0.07795922458171844 | [Val] Loss: 0.08900618553161621\n",
      "Epoch 172 -> [Train] Loss: 0.07796303927898407 | [Val] Loss: 0.0889313817024231\n",
      "Epoch 173 -> [Train] Loss: 0.07794927060604095 | [Val] Loss: 0.08916927129030228\n",
      "Epoch 174 -> [Train] Loss: 0.07791957259178162 | [Val] Loss: 0.08905317634344101\n",
      "Epoch 175 -> [Train] Loss: 0.07793565839529037 | [Val] Loss: 0.08916819095611572\n",
      "Epoch 176 -> [Train] Loss: 0.07788552343845367 | [Val] Loss: 0.08916769176721573\n",
      "Epoch 177 -> [Train] Loss: 0.07789961248636246 | [Val] Loss: 0.0892067402601242\n",
      "Epoch 178 -> [Train] Loss: 0.07789426296949387 | [Val] Loss: 0.08928695321083069\n",
      "Epoch 179 -> [Train] Loss: 0.0778828039765358 | [Val] Loss: 0.08905043452978134\n",
      "Epoch 180 -> [Train] Loss: 0.07787580788135529 | [Val] Loss: 0.08948144316673279\n",
      "Epoch 181 -> [Train] Loss: 0.07786588370800018 | [Val] Loss: 0.08901701867580414\n",
      "Epoch 182 -> [Train] Loss: 0.07785625755786896 | [Val] Loss: 0.08917298913002014\n",
      "Epoch 183 -> [Train] Loss: 0.07785391062498093 | [Val] Loss: 0.08907333761453629\n",
      "Epoch 184 -> [Train] Loss: 0.07784480601549149 | [Val] Loss: 0.0892728939652443\n",
      "Epoch 185 -> [Train] Loss: 0.0778181180357933 | [Val] Loss: 0.08924918621778488\n",
      "Epoch 186 -> [Train] Loss: 0.07781792432069778 | [Val] Loss: 0.08934042602777481\n",
      "Epoch 187 -> [Train] Loss: 0.07782686501741409 | [Val] Loss: 0.08914805203676224\n",
      "Epoch 188 -> [Train] Loss: 0.07782624661922455 | [Val] Loss: 0.0889340415596962\n",
      "Epoch 189 -> [Train] Loss: 0.07780256122350693 | [Val] Loss: 0.08890540897846222\n",
      "Epoch 190 -> [Train] Loss: 0.0777880996465683 | [Val] Loss: 0.08904614299535751\n",
      "Epoch 191 -> [Train] Loss: 0.07775957137346268 | [Val] Loss: 0.08914772421121597\n",
      "Epoch 192 -> [Train] Loss: 0.07775329053401947 | [Val] Loss: 0.08908344060182571\n",
      "Epoch 193 -> [Train] Loss: 0.07776886224746704 | [Val] Loss: 0.08910708874464035\n",
      "Epoch 194 -> [Train] Loss: 0.07774322479963303 | [Val] Loss: 0.08916396647691727\n",
      "Epoch 195 -> [Train] Loss: 0.07777095586061478 | [Val] Loss: 0.08901771157979965\n",
      "Epoch 196 -> [Train] Loss: 0.07775163650512695 | [Val] Loss: 0.08929707109928131\n",
      "Epoch 197 -> [Train] Loss: 0.07774469256401062 | [Val] Loss: 0.08915342390537262\n",
      "Epoch 198 -> [Train] Loss: 0.07775937020778656 | [Val] Loss: 0.08876644819974899\n",
      "Epoch 199 -> [Train] Loss: 0.07773343473672867 | [Val] Loss: 0.08907632529735565\n",
      "Epoch 200 -> [Train] Loss: 0.07776152342557907 | [Val] Loss: 0.08906684070825577\n",
      "Epoch 201 -> [Train] Loss: 0.07769855111837387 | [Val] Loss: 0.08907726407051086\n",
      "Epoch 202 -> [Train] Loss: 0.07769447565078735 | [Val] Loss: 0.08896247297525406\n",
      "Epoch 203 -> [Train] Loss: 0.07769821584224701 | [Val] Loss: 0.08924072235822678\n",
      "Epoch 204 -> [Train] Loss: 0.07769328355789185 | [Val] Loss: 0.08870088309049606\n",
      "Epoch 205 -> [Train] Loss: 0.07767527550458908 | [Val] Loss: 0.08902651071548462\n",
      "Epoch 206 -> [Train] Loss: 0.07767488062381744 | [Val] Loss: 0.0891440212726593\n",
      "Epoch 207 -> [Train] Loss: 0.07762917876243591 | [Val] Loss: 0.0889950841665268\n",
      "Epoch 208 -> [Train] Loss: 0.07762717455625534 | [Val] Loss: 0.08909152448177338\n",
      "Epoch 209 -> [Train] Loss: 0.07764799892902374 | [Val] Loss: 0.0889575332403183\n",
      "Epoch 210 -> [Train] Loss: 0.0776122435927391 | [Val] Loss: 0.08906235545873642\n",
      "Epoch 211 -> [Train] Loss: 0.07765772938728333 | [Val] Loss: 0.0889672115445137\n",
      "Epoch 212 -> [Train] Loss: 0.07762064039707184 | [Val] Loss: 0.08907321840524673\n",
      "Epoch 213 -> [Train] Loss: 0.07763338834047318 | [Val] Loss: 0.08894459903240204\n",
      "Epoch 214 -> [Train] Loss: 0.07756330817937851 | [Val] Loss: 0.08900279551744461\n",
      "Epoch 215 -> [Train] Loss: 0.07755212485790253 | [Val] Loss: 0.08902209997177124\n",
      "Epoch 216 -> [Train] Loss: 0.07754077017307281 | [Val] Loss: 0.08919475972652435\n",
      "Epoch 217 -> [Train] Loss: 0.07752740383148193 | [Val] Loss: 0.08912213146686554\n",
      "Epoch 218 -> [Train] Loss: 0.07749751955270767 | [Val] Loss: 0.08935558795928955\n",
      "Epoch 219 -> [Train] Loss: 0.07748725265264511 | [Val] Loss: 0.08920653164386749\n",
      "Epoch 220 -> [Train] Loss: 0.07747873663902283 | [Val] Loss: 0.08934266865253448\n",
      "Epoch 221 -> [Train] Loss: 0.07747907191514969 | [Val] Loss: 0.08922262489795685\n",
      "Epoch 222 -> [Train] Loss: 0.07746195048093796 | [Val] Loss: 0.08932933211326599\n",
      "Epoch 223 -> [Train] Loss: 0.07744253426790237 | [Val] Loss: 0.08923359215259552\n",
      "Epoch 224 -> [Train] Loss: 0.0774221196770668 | [Val] Loss: 0.08947785943746567\n",
      "Epoch 225 -> [Train] Loss: 0.07743161171674728 | [Val] Loss: 0.08924111723899841\n",
      "Epoch 226 -> [Train] Loss: 0.07738438248634338 | [Val] Loss: 0.08950447291135788\n",
      "Epoch 227 -> [Train] Loss: 0.07738111913204193 | [Val] Loss: 0.08951160311698914\n",
      "Epoch 228 -> [Train] Loss: 0.07735133916139603 | [Val] Loss: 0.08966474235057831\n",
      "Epoch 229 -> [Train] Loss: 0.07734121382236481 | [Val] Loss: 0.08955378830432892\n",
      "Epoch 230 -> [Train] Loss: 0.07735252380371094 | [Val] Loss: 0.08956615626811981\n",
      "Epoch 231 -> [Train] Loss: 0.0773414820432663 | [Val] Loss: 0.08978813886642456\n",
      "Epoch 232 -> [Train] Loss: 0.07729724049568176 | [Val] Loss: 0.08964936435222626\n",
      "Epoch 233 -> [Train] Loss: 0.0772993192076683 | [Val] Loss: 0.08985916525125504\n",
      "Epoch 234 -> [Train] Loss: 0.07729633897542953 | [Val] Loss: 0.08963633328676224\n",
      "Epoch 235 -> [Train] Loss: 0.07728319615125656 | [Val] Loss: 0.08979751169681549\n",
      "Epoch 236 -> [Train] Loss: 0.0772930234670639 | [Val] Loss: 0.0897066593170166\n",
      "Epoch 237 -> [Train] Loss: 0.0772467851638794 | [Val] Loss: 0.08972607553005219\n",
      "Epoch 238 -> [Train] Loss: 0.0772584080696106 | [Val] Loss: 0.08960240334272385\n",
      "Epoch 239 -> [Train] Loss: 0.07723461091518402 | [Val] Loss: 0.08947579562664032\n",
      "Epoch 240 -> [Train] Loss: 0.07724403589963913 | [Val] Loss: 0.08969547599554062\n",
      "Epoch 241 -> [Train] Loss: 0.07723070681095123 | [Val] Loss: 0.08948738873004913\n",
      "Epoch 242 -> [Train] Loss: 0.07724358886480331 | [Val] Loss: 0.08930153399705887\n",
      "Epoch 243 -> [Train] Loss: 0.07721483707427979 | [Val] Loss: 0.08920532464981079\n",
      "Epoch 244 -> [Train] Loss: 0.07721083611249924 | [Val] Loss: 0.08915185928344727\n",
      "Epoch 245 -> [Train] Loss: 0.07718943804502487 | [Val] Loss: 0.08924680203199387\n",
      "Epoch 246 -> [Train] Loss: 0.07720135897397995 | [Val] Loss: 0.0892883762717247\n",
      "Epoch 247 -> [Train] Loss: 0.07717695832252502 | [Val] Loss: 0.08931366354227066\n",
      "Epoch 248 -> [Train] Loss: 0.07717283070087433 | [Val] Loss: 0.08954931795597076\n",
      "Epoch 249 -> [Train] Loss: 0.07718115299940109 | [Val] Loss: 0.0894438847899437\n",
      "Epoch 250 -> [Train] Loss: 0.07718060910701752 | [Val] Loss: 0.08930014818906784\n",
      "Epoch 251 -> [Train] Loss: 0.07715421915054321 | [Val] Loss: 0.08957366645336151\n",
      "Epoch 252 -> [Train] Loss: 0.07713469117879868 | [Val] Loss: 0.0893135666847229\n",
      "Epoch 253 -> [Train] Loss: 0.07710576057434082 | [Val] Loss: 0.08949883282184601\n",
      "Epoch 254 -> [Train] Loss: 0.0771307498216629 | [Val] Loss: 0.08934016525745392\n",
      "Epoch 255 -> [Train] Loss: 0.07714159041643143 | [Val] Loss: 0.0895005464553833\n",
      "Epoch 256 -> [Train] Loss: 0.07711464166641235 | [Val] Loss: 0.0894271582365036\n",
      "Epoch 257 -> [Train] Loss: 0.07709722965955734 | [Val] Loss: 0.08933186531066895\n",
      "Epoch 258 -> [Train] Loss: 0.07709934562444687 | [Val] Loss: 0.08948366343975067\n",
      "Epoch 259 -> [Train] Loss: 0.07707394659519196 | [Val] Loss: 0.08941363543272018\n",
      "Epoch 260 -> [Train] Loss: 0.07708744704723358 | [Val] Loss: 0.08940012753009796\n",
      "Epoch 261 -> [Train] Loss: 0.077048160135746 | [Val] Loss: 0.08919968456029892\n",
      "Epoch 262 -> [Train] Loss: 0.07704288512468338 | [Val] Loss: 0.08946961909532547\n",
      "Epoch 263 -> [Train] Loss: 0.07704335451126099 | [Val] Loss: 0.08929438889026642\n",
      "Epoch 264 -> [Train] Loss: 0.07704125344753265 | [Val] Loss: 0.08934465050697327\n",
      "Epoch 265 -> [Train] Loss: 0.07701202481985092 | [Val] Loss: 0.08922216296195984\n",
      "Epoch 266 -> [Train] Loss: 0.07703828811645508 | [Val] Loss: 0.08924084901809692\n",
      "Epoch 267 -> [Train] Loss: 0.07700922340154648 | [Val] Loss: 0.08930905908346176\n",
      "Epoch 268 -> [Train] Loss: 0.07700251787900925 | [Val] Loss: 0.08930353075265884\n",
      "Epoch 269 -> [Train] Loss: 0.07699497044086456 | [Val] Loss: 0.08917712420225143\n",
      "Epoch 270 -> [Train] Loss: 0.0769897922873497 | [Val] Loss: 0.0892249271273613\n",
      "Epoch 271 -> [Train] Loss: 0.07697555422782898 | [Val] Loss: 0.08919607102870941\n",
      "Epoch 272 -> [Train] Loss: 0.07699386030435562 | [Val] Loss: 0.08940039575099945\n",
      "Epoch 273 -> [Train] Loss: 0.07699073851108551 | [Val] Loss: 0.08957509696483612\n",
      "Epoch 274 -> [Train] Loss: 0.07698711007833481 | [Val] Loss: 0.0893387496471405\n",
      "Epoch 275 -> [Train] Loss: 0.07694981247186661 | [Val] Loss: 0.08932770788669586\n",
      "Epoch 276 -> [Train] Loss: 0.076985202729702 | [Val] Loss: 0.0893302783370018\n",
      "Epoch 277 -> [Train] Loss: 0.07697319984436035 | [Val] Loss: 0.08943867683410645\n",
      "Epoch 278 -> [Train] Loss: 0.0769636407494545 | [Val] Loss: 0.08944125473499298\n",
      "Epoch 279 -> [Train] Loss: 0.0769505500793457 | [Val] Loss: 0.08933451771736145\n",
      "Epoch 280 -> [Train] Loss: 0.07691982388496399 | [Val] Loss: 0.08944117277860641\n",
      "Epoch 281 -> [Train] Loss: 0.07691793888807297 | [Val] Loss: 0.08938005566596985\n",
      "Epoch 282 -> [Train] Loss: 0.07692839205265045 | [Val] Loss: 0.08957619965076447\n",
      "Epoch 283 -> [Train] Loss: 0.07692378759384155 | [Val] Loss: 0.08951590955257416\n",
      "Epoch 284 -> [Train] Loss: 0.07690171152353287 | [Val] Loss: 0.0893048495054245\n",
      "Epoch 285 -> [Train] Loss: 0.07690950483083725 | [Val] Loss: 0.08932028710842133\n",
      "Epoch 286 -> [Train] Loss: 0.07689204812049866 | [Val] Loss: 0.08961526304483414\n",
      "Epoch 287 -> [Train] Loss: 0.07689572870731354 | [Val] Loss: 0.08946890383958817\n",
      "Epoch 288 -> [Train] Loss: 0.07686367630958557 | [Val] Loss: 0.0895846039056778\n",
      "Epoch 289 -> [Train] Loss: 0.07685212045907974 | [Val] Loss: 0.08936187624931335\n",
      "Epoch 290 -> [Train] Loss: 0.0768471211194992 | [Val] Loss: 0.08975455909967422\n",
      "Epoch 291 -> [Train] Loss: 0.07686062902212143 | [Val] Loss: 0.08950196206569672\n",
      "Epoch 292 -> [Train] Loss: 0.07683313637971878 | [Val] Loss: 0.08956997096538544\n",
      "Epoch 293 -> [Train] Loss: 0.07684358209371567 | [Val] Loss: 0.08961279690265656\n",
      "Epoch 294 -> [Train] Loss: 0.07684480398893356 | [Val] Loss: 0.08975733816623688\n",
      "Epoch 295 -> [Train] Loss: 0.07677454501390457 | [Val] Loss: 0.08965189009904861\n",
      "Epoch 296 -> [Train] Loss: 0.07681608945131302 | [Val] Loss: 0.08988107740879059\n",
      "Epoch 297 -> [Train] Loss: 0.07681193202733994 | [Val] Loss: 0.0899001881480217\n",
      "Epoch 298 -> [Train] Loss: 0.076804980635643 | [Val] Loss: 0.08987630903720856\n",
      "Epoch 299 -> [Train] Loss: 0.07679177075624466 | [Val] Loss: 0.0900084599852562\n",
      "Epoch 300 -> [Train] Loss: 0.07677637040615082 | [Val] Loss: 0.0899900496006012\n",
      "Epoch 301 -> [Train] Loss: 0.07676135003566742 | [Val] Loss: 0.0899186059832573\n",
      "Epoch 302 -> [Train] Loss: 0.07674280554056168 | [Val] Loss: 0.0900682881474495\n",
      "Epoch 303 -> [Train] Loss: 0.07675619423389435 | [Val] Loss: 0.08993769437074661\n",
      "Epoch 304 -> [Train] Loss: 0.07670777291059494 | [Val] Loss: 0.09011780470609665\n",
      "Epoch 305 -> [Train] Loss: 0.07674120366573334 | [Val] Loss: 0.08993560820817947\n",
      "Epoch 306 -> [Train] Loss: 0.07673519104719162 | [Val] Loss: 0.09011385589838028\n",
      "Epoch 307 -> [Train] Loss: 0.07675844430923462 | [Val] Loss: 0.09013480693101883\n",
      "Epoch 308 -> [Train] Loss: 0.0766868144273758 | [Val] Loss: 0.0900908038020134\n",
      "Epoch 309 -> [Train] Loss: 0.07672301679849625 | [Val] Loss: 0.09005770832300186\n",
      "Epoch 310 -> [Train] Loss: 0.07674333453178406 | [Val] Loss: 0.09013447910547256\n",
      "Epoch 311 -> [Train] Loss: 0.07672284543514252 | [Val] Loss: 0.09008333832025528\n",
      "Epoch 312 -> [Train] Loss: 0.0766914039850235 | [Val] Loss: 0.09015120565891266\n",
      "Epoch 313 -> [Train] Loss: 0.07669702172279358 | [Val] Loss: 0.09012296795845032\n",
      "Epoch 314 -> [Train] Loss: 0.07667909562587738 | [Val] Loss: 0.08990225940942764\n",
      "Epoch 315 -> [Train] Loss: 0.07668140530586243 | [Val] Loss: 0.09024639427661896\n",
      "Epoch 316 -> [Train] Loss: 0.07667886465787888 | [Val] Loss: 0.09006543457508087\n",
      "Epoch 317 -> [Train] Loss: 0.07669492065906525 | [Val] Loss: 0.09018772095441818\n",
      "Epoch 318 -> [Train] Loss: 0.07667718827724457 | [Val] Loss: 0.08999893814325333\n",
      "Epoch 319 -> [Train] Loss: 0.07664889097213745 | [Val] Loss: 0.09018640220165253\n",
      "Epoch 320 -> [Train] Loss: 0.07663712650537491 | [Val] Loss: 0.08996424823999405\n",
      "Epoch 321 -> [Train] Loss: 0.07662812620401382 | [Val] Loss: 0.09008068591356277\n",
      "Epoch 322 -> [Train] Loss: 0.07662920653820038 | [Val] Loss: 0.09000245481729507\n",
      "Epoch 323 -> [Train] Loss: 0.07661086320877075 | [Val] Loss: 0.090212382376194\n",
      "Epoch 324 -> [Train] Loss: 0.07663413137197495 | [Val] Loss: 0.08990711718797684\n",
      "Epoch 325 -> [Train] Loss: 0.07661587744951248 | [Val] Loss: 0.09005413204431534\n",
      "Epoch 326 -> [Train] Loss: 0.07660244405269623 | [Val] Loss: 0.0901205763220787\n",
      "Epoch 327 -> [Train] Loss: 0.07658969610929489 | [Val] Loss: 0.09019291400909424\n",
      "Epoch 328 -> [Train] Loss: 0.07659013569355011 | [Val] Loss: 0.09000062942504883\n",
      "Epoch 329 -> [Train] Loss: 0.07660016417503357 | [Val] Loss: 0.09027980268001556\n",
      "Epoch 330 -> [Train] Loss: 0.07658544182777405 | [Val] Loss: 0.09047137200832367\n",
      "Epoch 331 -> [Train] Loss: 0.07656160742044449 | [Val] Loss: 0.09026303887367249\n",
      "Epoch 332 -> [Train] Loss: 0.07654398679733276 | [Val] Loss: 0.09046883881092072\n",
      "Epoch 333 -> [Train] Loss: 0.07655682414770126 | [Val] Loss: 0.09036912024021149\n",
      "Epoch 334 -> [Train] Loss: 0.07654117047786713 | [Val] Loss: 0.09031957387924194\n",
      "Epoch 335 -> [Train] Loss: 0.07655250281095505 | [Val] Loss: 0.09054522961378098\n",
      "Epoch 336 -> [Train] Loss: 0.07655297964811325 | [Val] Loss: 0.0905250832438469\n",
      "Epoch 337 -> [Train] Loss: 0.07652482390403748 | [Val] Loss: 0.09073282033205032\n",
      "Epoch 338 -> [Train] Loss: 0.07651299238204956 | [Val] Loss: 0.0903870165348053\n",
      "Epoch 339 -> [Train] Loss: 0.07651318609714508 | [Val] Loss: 0.09046638756990433\n",
      "Epoch 340 -> [Train] Loss: 0.07649733871221542 | [Val] Loss: 0.09072200208902359\n",
      "Epoch 341 -> [Train] Loss: 0.07651212811470032 | [Val] Loss: 0.09063947945833206\n",
      "Epoch 342 -> [Train] Loss: 0.07650411874055862 | [Val] Loss: 0.09087219834327698\n",
      "Epoch 343 -> [Train] Loss: 0.07647047191858292 | [Val] Loss: 0.0907079353928566\n",
      "Epoch 344 -> [Train] Loss: 0.07647380977869034 | [Val] Loss: 0.09060239046812057\n",
      "Epoch 345 -> [Train] Loss: 0.07646169513463974 | [Val] Loss: 0.09063065052032471\n",
      "Epoch 346 -> [Train] Loss: 0.07645821571350098 | [Val] Loss: 0.09084265679121017\n",
      "Epoch 347 -> [Train] Loss: 0.07647676020860672 | [Val] Loss: 0.09065820276737213\n",
      "Epoch 348 -> [Train] Loss: 0.07644174993038177 | [Val] Loss: 0.0907648503780365\n",
      "Epoch 349 -> [Train] Loss: 0.07645256072282791 | [Val] Loss: 0.09101869910955429\n",
      "Epoch 350 -> [Train] Loss: 0.07644166797399521 | [Val] Loss: 0.09080654382705688\n",
      "Epoch 351 -> [Train] Loss: 0.07644587010145187 | [Val] Loss: 0.09058936685323715\n",
      "Epoch 352 -> [Train] Loss: 0.07640991359949112 | [Val] Loss: 0.0908450037240982\n",
      "Epoch 353 -> [Train] Loss: 0.07645729929208755 | [Val] Loss: 0.09090101718902588\n",
      "Epoch 354 -> [Train] Loss: 0.07643505930900574 | [Val] Loss: 0.09071879088878632\n",
      "Epoch 355 -> [Train] Loss: 0.07643012702465057 | [Val] Loss: 0.09057953208684921\n",
      "Epoch 356 -> [Train] Loss: 0.07641419768333435 | [Val] Loss: 0.09059024602174759\n",
      "Epoch 357 -> [Train] Loss: 0.07640797644853592 | [Val] Loss: 0.09065884351730347\n",
      "Epoch 358 -> [Train] Loss: 0.076370969414711 | [Val] Loss: 0.09079767018556595\n",
      "Epoch 359 -> [Train] Loss: 0.07638633996248245 | [Val] Loss: 0.0910118892788887\n",
      "Epoch 360 -> [Train] Loss: 0.07637535035610199 | [Val] Loss: 0.09081599116325378\n",
      "Epoch 361 -> [Train] Loss: 0.07639838010072708 | [Val] Loss: 0.09085973352193832\n",
      "Epoch 362 -> [Train] Loss: 0.07639435678720474 | [Val] Loss: 0.09110726416110992\n",
      "Epoch 363 -> [Train] Loss: 0.07638824731111526 | [Val] Loss: 0.09097865223884583\n",
      "Epoch 364 -> [Train] Loss: 0.07637834548950195 | [Val] Loss: 0.09099410474300385\n",
      "Epoch 365 -> [Train] Loss: 0.07637543976306915 | [Val] Loss: 0.0907098725438118\n",
      "Epoch 366 -> [Train] Loss: 0.07634037733078003 | [Val] Loss: 0.09116579592227936\n",
      "Epoch 367 -> [Train] Loss: 0.07635422796010971 | [Val] Loss: 0.090918630361557\n",
      "Epoch 368 -> [Train] Loss: 0.0763571485877037 | [Val] Loss: 0.09118233621120453\n",
      "Epoch 369 -> [Train] Loss: 0.07634805142879486 | [Val] Loss: 0.09097261726856232\n",
      "Epoch 370 -> [Train] Loss: 0.0763268768787384 | [Val] Loss: 0.09092889726161957\n",
      "Epoch 371 -> [Train] Loss: 0.07633674144744873 | [Val] Loss: 0.0909222736954689\n",
      "Epoch 372 -> [Train] Loss: 0.07633260637521744 | [Val] Loss: 0.09065321832895279\n",
      "Epoch 373 -> [Train] Loss: 0.07634016871452332 | [Val] Loss: 0.0908011943101883\n",
      "Epoch 374 -> [Train] Loss: 0.076333187520504 | [Val] Loss: 0.09089099615812302\n",
      "Epoch 375 -> [Train] Loss: 0.0763508677482605 | [Val] Loss: 0.09102297574281693\n",
      "Epoch 376 -> [Train] Loss: 0.07631883770227432 | [Val] Loss: 0.09074173122644424\n",
      "Epoch 377 -> [Train] Loss: 0.07626928389072418 | [Val] Loss: 0.0906972587108612\n",
      "Epoch 378 -> [Train] Loss: 0.07626666873693466 | [Val] Loss: 0.09088215976953506\n",
      "Epoch 379 -> [Train] Loss: 0.07627607882022858 | [Val] Loss: 0.09078656882047653\n",
      "Epoch 380 -> [Train] Loss: 0.07627221196889877 | [Val] Loss: 0.09067218750715256\n",
      "Epoch 381 -> [Train] Loss: 0.0762593075633049 | [Val] Loss: 0.09057445079088211\n",
      "Epoch 382 -> [Train] Loss: 0.07627338171005249 | [Val] Loss: 0.09067025780677795\n",
      "Epoch 383 -> [Train] Loss: 0.07626131176948547 | [Val] Loss: 0.0907215103507042\n",
      "Epoch 384 -> [Train] Loss: 0.07626473903656006 | [Val] Loss: 0.0907340943813324\n",
      "Epoch 385 -> [Train] Loss: 0.07624449580907822 | [Val] Loss: 0.0908144861459732\n",
      "Epoch 386 -> [Train] Loss: 0.076229989528656 | [Val] Loss: 0.0906396210193634\n",
      "Epoch 387 -> [Train] Loss: 0.07626374810934067 | [Val] Loss: 0.0906718298792839\n",
      "Epoch 388 -> [Train] Loss: 0.07624469697475433 | [Val] Loss: 0.0908098891377449\n",
      "Epoch 389 -> [Train] Loss: 0.07621380686759949 | [Val] Loss: 0.09078928828239441\n",
      "Epoch 390 -> [Train] Loss: 0.07621904462575912 | [Val] Loss: 0.09058046340942383\n",
      "Epoch 391 -> [Train] Loss: 0.07622366398572922 | [Val] Loss: 0.09073932468891144\n",
      "Epoch 392 -> [Train] Loss: 0.07621850073337555 | [Val] Loss: 0.09043046832084656\n",
      "Epoch 393 -> [Train] Loss: 0.07620947808027267 | [Val] Loss: 0.09044681489467621\n",
      "Epoch 394 -> [Train] Loss: 0.07618934661149979 | [Val] Loss: 0.09052655100822449\n",
      "Epoch 395 -> [Train] Loss: 0.07623084634542465 | [Val] Loss: 0.09043146669864655\n",
      "Epoch 396 -> [Train] Loss: 0.07620185613632202 | [Val] Loss: 0.09035661816596985\n",
      "Epoch 397 -> [Train] Loss: 0.07618249207735062 | [Val] Loss: 0.0903242900967598\n",
      "Epoch 398 -> [Train] Loss: 0.07618474960327148 | [Val] Loss: 0.09068666398525238\n",
      "Epoch 399 -> [Train] Loss: 0.07616636157035828 | [Val] Loss: 0.09053627401590347\n",
      "Epoch 400 -> [Train] Loss: 0.07617295533418655 | [Val] Loss: 0.09062173962593079\n",
      "Epoch 401 -> [Train] Loss: 0.07618855684995651 | [Val] Loss: 0.09029152244329453\n",
      "Epoch 402 -> [Train] Loss: 0.07616711407899857 | [Val] Loss: 0.09035490453243256\n",
      "Epoch 403 -> [Train] Loss: 0.07615669816732407 | [Val] Loss: 0.09053990244865417\n",
      "Epoch 404 -> [Train] Loss: 0.07618186622858047 | [Val] Loss: 0.09041905403137207\n",
      "Epoch 405 -> [Train] Loss: 0.07615877687931061 | [Val] Loss: 0.0902147889137268\n",
      "Epoch 406 -> [Train] Loss: 0.07613582909107208 | [Val] Loss: 0.09030874073505402\n",
      "Epoch 407 -> [Train] Loss: 0.07614778727293015 | [Val] Loss: 0.09057005494832993\n",
      "Epoch 408 -> [Train] Loss: 0.07612738758325577 | [Val] Loss: 0.090231753885746\n",
      "Epoch 409 -> [Train] Loss: 0.07611846178770065 | [Val] Loss: 0.09027060866355896\n",
      "Epoch 410 -> [Train] Loss: 0.07612698525190353 | [Val] Loss: 0.09038770198822021\n",
      "Epoch 411 -> [Train] Loss: 0.07614642381668091 | [Val] Loss: 0.09046421200037003\n",
      "Epoch 412 -> [Train] Loss: 0.07612568885087967 | [Val] Loss: 0.09026562422513962\n",
      "Epoch 413 -> [Train] Loss: 0.0761173814535141 | [Val] Loss: 0.09029416739940643\n",
      "Epoch 414 -> [Train] Loss: 0.07612752914428711 | [Val] Loss: 0.09009267389774323\n",
      "Epoch 415 -> [Train] Loss: 0.07613170146942139 | [Val] Loss: 0.09030173718929291\n",
      "Epoch 416 -> [Train] Loss: 0.07614699751138687 | [Val] Loss: 0.09026698023080826\n",
      "Epoch 417 -> [Train] Loss: 0.0761188417673111 | [Val] Loss: 0.09043805301189423\n",
      "Epoch 418 -> [Train] Loss: 0.07612524926662445 | [Val] Loss: 0.09033634513616562\n",
      "Epoch 419 -> [Train] Loss: 0.07611963152885437 | [Val] Loss: 0.09016022831201553\n",
      "Epoch 420 -> [Train] Loss: 0.0760921835899353 | [Val] Loss: 0.09042837470769882\n",
      "Epoch 421 -> [Train] Loss: 0.07607774436473846 | [Val] Loss: 0.09028524905443192\n",
      "Epoch 422 -> [Train] Loss: 0.07607565075159073 | [Val] Loss: 0.09045474231243134\n",
      "Epoch 423 -> [Train] Loss: 0.07605573534965515 | [Val] Loss: 0.09044468402862549\n",
      "Epoch 424 -> [Train] Loss: 0.07606705278158188 | [Val] Loss: 0.09025957435369492\n",
      "Epoch 425 -> [Train] Loss: 0.07602996379137039 | [Val] Loss: 0.0903141126036644\n",
      "Epoch 426 -> [Train] Loss: 0.0760200098156929 | [Val] Loss: 0.09036441147327423\n",
      "Epoch 427 -> [Train] Loss: 0.07602575421333313 | [Val] Loss: 0.09057247638702393\n",
      "Epoch 428 -> [Train] Loss: 0.07604268193244934 | [Val] Loss: 0.09055269509553909\n",
      "Epoch 429 -> [Train] Loss: 0.07600457221269608 | [Val] Loss: 0.09043145924806595\n",
      "Epoch 430 -> [Train] Loss: 0.0760306566953659 | [Val] Loss: 0.09050079435110092\n",
      "Epoch 431 -> [Train] Loss: 0.07602348923683167 | [Val] Loss: 0.09024810045957565\n",
      "Epoch 432 -> [Train] Loss: 0.07601159811019897 | [Val] Loss: 0.09058086574077606\n",
      "Epoch 433 -> [Train] Loss: 0.0760287418961525 | [Val] Loss: 0.09022980183362961\n",
      "Epoch 434 -> [Train] Loss: 0.07597912847995758 | [Val] Loss: 0.09031440317630768\n",
      "Epoch 435 -> [Train] Loss: 0.07597018033266068 | [Val] Loss: 0.09036611020565033\n",
      "Epoch 436 -> [Train] Loss: 0.07599159330129623 | [Val] Loss: 0.09037897735834122\n",
      "Epoch 437 -> [Train] Loss: 0.0759657770395279 | [Val] Loss: 0.09025107324123383\n",
      "Epoch 438 -> [Train] Loss: 0.07595881819725037 | [Val] Loss: 0.09031843394041061\n",
      "Epoch 439 -> [Train] Loss: 0.07598801702260971 | [Val] Loss: 0.09036510437726974\n",
      "Epoch 440 -> [Train] Loss: 0.07596072554588318 | [Val] Loss: 0.09042268246412277\n",
      "Epoch 441 -> [Train] Loss: 0.07599761337041855 | [Val] Loss: 0.09027005732059479\n",
      "Epoch 442 -> [Train] Loss: 0.07594660669565201 | [Val] Loss: 0.09050358831882477\n",
      "Epoch 443 -> [Train] Loss: 0.07596052438020706 | [Val] Loss: 0.09062526375055313\n",
      "Epoch 444 -> [Train] Loss: 0.07593655586242676 | [Val] Loss: 0.09041687101125717\n",
      "Epoch 445 -> [Train] Loss: 0.07593302428722382 | [Val] Loss: 0.09038164466619492\n",
      "Epoch 446 -> [Train] Loss: 0.07594820857048035 | [Val] Loss: 0.09055541455745697\n",
      "Epoch 447 -> [Train] Loss: 0.07593702524900436 | [Val] Loss: 0.09044015407562256\n",
      "Epoch 448 -> [Train] Loss: 0.07592245936393738 | [Val] Loss: 0.09039098024368286\n",
      "Epoch 449 -> [Train] Loss: 0.07594490796327591 | [Val] Loss: 0.09044040739536285\n",
      "Epoch 450 -> [Train] Loss: 0.07597089558839798 | [Val] Loss: 0.09044462442398071\n",
      "Epoch 451 -> [Train] Loss: 0.07596465200185776 | [Val] Loss: 0.09066890180110931\n",
      "Epoch 452 -> [Train] Loss: 0.07593102753162384 | [Val] Loss: 0.0904693529009819\n",
      "Epoch 453 -> [Train] Loss: 0.07594926655292511 | [Val] Loss: 0.09059056639671326\n",
      "Epoch 454 -> [Train] Loss: 0.075943723320961 | [Val] Loss: 0.09050864726305008\n",
      "Epoch 455 -> [Train] Loss: 0.07593036442995071 | [Val] Loss: 0.09055235236883163\n",
      "Epoch 456 -> [Train] Loss: 0.07592608034610748 | [Val] Loss: 0.09076914191246033\n",
      "Epoch 457 -> [Train] Loss: 0.07593356817960739 | [Val] Loss: 0.09057604521512985\n",
      "Epoch 458 -> [Train] Loss: 0.07591236382722855 | [Val] Loss: 0.09073325246572495\n",
      "Epoch 459 -> [Train] Loss: 0.07592542469501495 | [Val] Loss: 0.09055985510349274\n",
      "Epoch 460 -> [Train] Loss: 0.07591668516397476 | [Val] Loss: 0.09044842422008514\n",
      "Epoch 461 -> [Train] Loss: 0.07593058049678802 | [Val] Loss: 0.09046997874975204\n",
      "Epoch 462 -> [Train] Loss: 0.075930655002594 | [Val] Loss: 0.09073016792535782\n",
      "Epoch 463 -> [Train] Loss: 0.07594402134418488 | [Val] Loss: 0.09067244827747345\n",
      "Epoch 464 -> [Train] Loss: 0.07589395344257355 | [Val] Loss: 0.09056457132101059\n",
      "Epoch 465 -> [Train] Loss: 0.07592922449111938 | [Val] Loss: 0.09069675207138062\n",
      "Epoch 466 -> [Train] Loss: 0.07590709626674652 | [Val] Loss: 0.09075918793678284\n",
      "Epoch 467 -> [Train] Loss: 0.07587075233459473 | [Val] Loss: 0.0907098799943924\n",
      "Epoch 468 -> [Train] Loss: 0.0758800059556961 | [Val] Loss: 0.0907321348786354\n",
      "Epoch 469 -> [Train] Loss: 0.07589516788721085 | [Val] Loss: 0.09070269763469696\n",
      "Epoch 470 -> [Train] Loss: 0.07589532434940338 | [Val] Loss: 0.09064380824565887\n",
      "Epoch 471 -> [Train] Loss: 0.0758904442191124 | [Val] Loss: 0.0906989648938179\n",
      "Epoch 472 -> [Train] Loss: 0.07588031888008118 | [Val] Loss: 0.09068634361028671\n",
      "Epoch 473 -> [Train] Loss: 0.07590702176094055 | [Val] Loss: 0.0909268707036972\n",
      "Epoch 474 -> [Train] Loss: 0.07588571310043335 | [Val] Loss: 0.09089811146259308\n",
      "Epoch 475 -> [Train] Loss: 0.0758906677365303 | [Val] Loss: 0.09062453359365463\n",
      "Epoch 476 -> [Train] Loss: 0.0758831650018692 | [Val] Loss: 0.09071572870016098\n",
      "Epoch 477 -> [Train] Loss: 0.07587973773479462 | [Val] Loss: 0.09089183062314987\n",
      "Epoch 478 -> [Train] Loss: 0.07586421072483063 | [Val] Loss: 0.09075216948986053\n",
      "Epoch 479 -> [Train] Loss: 0.07590708136558533 | [Val] Loss: 0.09071668982505798\n",
      "Epoch 480 -> [Train] Loss: 0.07587571442127228 | [Val] Loss: 0.09082312136888504\n",
      "Epoch 481 -> [Train] Loss: 0.07588733732700348 | [Val] Loss: 0.09083466231822968\n",
      "Epoch 482 -> [Train] Loss: 0.07586311548948288 | [Val] Loss: 0.09081895649433136\n",
      "Epoch 483 -> [Train] Loss: 0.07584606111049652 | [Val] Loss: 0.09063413739204407\n",
      "Epoch 484 -> [Train] Loss: 0.0758494958281517 | [Val] Loss: 0.09083102643489838\n",
      "Epoch 485 -> [Train] Loss: 0.07582024484872818 | [Val] Loss: 0.09054793417453766\n",
      "Epoch 486 -> [Train] Loss: 0.07582409679889679 | [Val] Loss: 0.0907180979847908\n",
      "Epoch 487 -> [Train] Loss: 0.07584282010793686 | [Val] Loss: 0.0907445102930069\n",
      "Epoch 488 -> [Train] Loss: 0.07580789178609848 | [Val] Loss: 0.09078557789325714\n",
      "Epoch 489 -> [Train] Loss: 0.07583655416965485 | [Val] Loss: 0.09074614942073822\n",
      "Epoch 490 -> [Train] Loss: 0.07582812756299973 | [Val] Loss: 0.09075292199850082\n",
      "Epoch 491 -> [Train] Loss: 0.07583394646644592 | [Val] Loss: 0.0908965989947319\n",
      "Epoch 492 -> [Train] Loss: 0.07583116739988327 | [Val] Loss: 0.09065485745668411\n",
      "Epoch 493 -> [Train] Loss: 0.07582064718008041 | [Val] Loss: 0.09094488620758057\n",
      "Epoch 494 -> [Train] Loss: 0.07582923769950867 | [Val] Loss: 0.09078212082386017\n",
      "Epoch 495 -> [Train] Loss: 0.0758034884929657 | [Val] Loss: 0.09086789935827255\n",
      "Epoch 496 -> [Train] Loss: 0.07581812888383865 | [Val] Loss: 0.09096343070268631\n",
      "Epoch 497 -> [Train] Loss: 0.07579807937145233 | [Val] Loss: 0.0908181294798851\n",
      "Epoch 498 -> [Train] Loss: 0.07581167668104172 | [Val] Loss: 0.09098177403211594\n",
      "Epoch 499 -> [Train] Loss: 0.07579606026411057 | [Val] Loss: 0.09080938249826431\n",
      "Epoch 500 -> [Train] Loss: 0.07578463852405548 | [Val] Loss: 0.09093651920557022\n",
      "Epoch 501 -> [Train] Loss: 0.07580535858869553 | [Val] Loss: 0.09069293737411499\n",
      "Epoch 502 -> [Train] Loss: 0.07578352093696594 | [Val] Loss: 0.09098824858665466\n",
      "Epoch 503 -> [Train] Loss: 0.07579022645950317 | [Val] Loss: 0.0910528376698494\n",
      "Epoch 504 -> [Train] Loss: 0.07578718662261963 | [Val] Loss: 0.09095068275928497\n",
      "Epoch 505 -> [Train] Loss: 0.07577279955148697 | [Val] Loss: 0.09095891565084457\n",
      "Epoch 506 -> [Train] Loss: 0.0757979229092598 | [Val] Loss: 0.09101919084787369\n",
      "Epoch 507 -> [Train] Loss: 0.07577481120824814 | [Val] Loss: 0.09100202471017838\n",
      "Epoch 508 -> [Train] Loss: 0.07578707486391068 | [Val] Loss: 0.0910247191786766\n",
      "Epoch 509 -> [Train] Loss: 0.07580135762691498 | [Val] Loss: 0.09115614742040634\n",
      "Epoch 510 -> [Train] Loss: 0.07575175911188126 | [Val] Loss: 0.09088170528411865\n",
      "Epoch 511 -> [Train] Loss: 0.07575493305921555 | [Val] Loss: 0.09089404344558716\n",
      "Epoch 512 -> [Train] Loss: 0.07577762007713318 | [Val] Loss: 0.09095290303230286\n",
      "Epoch 513 -> [Train] Loss: 0.0757547914981842 | [Val] Loss: 0.09106861799955368\n",
      "Epoch 514 -> [Train] Loss: 0.0757516473531723 | [Val] Loss: 0.09084843099117279\n",
      "Epoch 515 -> [Train] Loss: 0.0757569670677185 | [Val] Loss: 0.09123895317316055\n",
      "Epoch 516 -> [Train] Loss: 0.07577371597290039 | [Val] Loss: 0.09109361469745636\n",
      "Epoch 517 -> [Train] Loss: 0.07575103640556335 | [Val] Loss: 0.09116065502166748\n",
      "Epoch 518 -> [Train] Loss: 0.07575774937868118 | [Val] Loss: 0.09112893044948578\n",
      "Epoch 519 -> [Train] Loss: 0.07576777040958405 | [Val] Loss: 0.09131699055433273\n",
      "Epoch 520 -> [Train] Loss: 0.07573948800563812 | [Val] Loss: 0.09115394204854965\n",
      "Epoch 521 -> [Train] Loss: 0.07574430853128433 | [Val] Loss: 0.09130562841892242\n",
      "Epoch 522 -> [Train] Loss: 0.07571760565042496 | [Val] Loss: 0.09107912331819534\n",
      "Epoch 523 -> [Train] Loss: 0.07572545856237411 | [Val] Loss: 0.09090989828109741\n",
      "Epoch 524 -> [Train] Loss: 0.07570215314626694 | [Val] Loss: 0.09122125804424286\n",
      "Epoch 525 -> [Train] Loss: 0.07572344690561295 | [Val] Loss: 0.0912928357720375\n",
      "Epoch 526 -> [Train] Loss: 0.07573244720697403 | [Val] Loss: 0.09122034907341003\n",
      "Epoch 527 -> [Train] Loss: 0.07569467276334763 | [Val] Loss: 0.09132646769285202\n",
      "Epoch 528 -> [Train] Loss: 0.07570850849151611 | [Val] Loss: 0.09124542772769928\n",
      "Epoch 529 -> [Train] Loss: 0.07569704204797745 | [Val] Loss: 0.09123396873474121\n",
      "Epoch 530 -> [Train] Loss: 0.07573477178812027 | [Val] Loss: 0.090992271900177\n",
      "Epoch 531 -> [Train] Loss: 0.07569795101881027 | [Val] Loss: 0.0911693125963211\n",
      "Epoch 532 -> [Train] Loss: 0.07570164650678635 | [Val] Loss: 0.0910874530673027\n",
      "Epoch 533 -> [Train] Loss: 0.07571734488010406 | [Val] Loss: 0.09106214344501495\n",
      "Epoch 534 -> [Train] Loss: 0.07570939511060715 | [Val] Loss: 0.09117884188890457\n",
      "Epoch 535 -> [Train] Loss: 0.07568825036287308 | [Val] Loss: 0.0910988301038742\n",
      "Epoch 536 -> [Train] Loss: 0.07570371776819229 | [Val] Loss: 0.09117641299962997\n",
      "Epoch 537 -> [Train] Loss: 0.07568434625864029 | [Val] Loss: 0.09097956120967865\n",
      "Epoch 538 -> [Train] Loss: 0.07567562162876129 | [Val] Loss: 0.09104793518781662\n",
      "Epoch 539 -> [Train] Loss: 0.07567153126001358 | [Val] Loss: 0.09136860817670822\n",
      "Epoch 540 -> [Train] Loss: 0.07566608488559723 | [Val] Loss: 0.09122846275568008\n",
      "Epoch 541 -> [Train] Loss: 0.07569755613803864 | [Val] Loss: 0.09092935174703598\n",
      "Epoch 542 -> [Train] Loss: 0.07567659020423889 | [Val] Loss: 0.09120657294988632\n",
      "Epoch 543 -> [Train] Loss: 0.07565981149673462 | [Val] Loss: 0.09125488251447678\n",
      "Epoch 544 -> [Train] Loss: 0.07563941180706024 | [Val] Loss: 0.09127187728881836\n",
      "Epoch 545 -> [Train] Loss: 0.0756567195057869 | [Val] Loss: 0.09111122786998749\n",
      "Epoch 546 -> [Train] Loss: 0.07564417272806168 | [Val] Loss: 0.09121627360582352\n",
      "Epoch 547 -> [Train] Loss: 0.07563778758049011 | [Val] Loss: 0.09102670848369598\n",
      "Epoch 548 -> [Train] Loss: 0.07562833279371262 | [Val] Loss: 0.09140373766422272\n",
      "Epoch 549 -> [Train] Loss: 0.07565151154994965 | [Val] Loss: 0.09116329997777939\n",
      "Epoch 550 -> [Train] Loss: 0.07562071084976196 | [Val] Loss: 0.09133202582597733\n",
      "Epoch 551 -> [Train] Loss: 0.07561839371919632 | [Val] Loss: 0.09147495031356812\n",
      "Epoch 552 -> [Train] Loss: 0.07562321424484253 | [Val] Loss: 0.09139212220907211\n",
      "Epoch 553 -> [Train] Loss: 0.07566022872924805 | [Val] Loss: 0.09122374653816223\n",
      "Epoch 554 -> [Train] Loss: 0.07562609016895294 | [Val] Loss: 0.09128253906965256\n",
      "Epoch 555 -> [Train] Loss: 0.07562823593616486 | [Val] Loss: 0.09133964776992798\n",
      "Epoch 556 -> [Train] Loss: 0.0756101906299591 | [Val] Loss: 0.09121689200401306\n",
      "Epoch 557 -> [Train] Loss: 0.07560852915048599 | [Val] Loss: 0.09130529314279556\n",
      "Epoch 558 -> [Train] Loss: 0.07560817152261734 | [Val] Loss: 0.09118913114070892\n",
      "Epoch 559 -> [Train] Loss: 0.07560978829860687 | [Val] Loss: 0.09112761914730072\n",
      "Epoch 560 -> [Train] Loss: 0.07559988647699356 | [Val] Loss: 0.09127385914325714\n",
      "Epoch 561 -> [Train] Loss: 0.07562031596899033 | [Val] Loss: 0.0909922644495964\n",
      "Epoch 562 -> [Train] Loss: 0.07559865713119507 | [Val] Loss: 0.0912591889500618\n",
      "Epoch 563 -> [Train] Loss: 0.07559775561094284 | [Val] Loss: 0.09142256528139114\n",
      "Epoch 564 -> [Train] Loss: 0.07559090852737427 | [Val] Loss: 0.09110836684703827\n",
      "Epoch 565 -> [Train] Loss: 0.07557249069213867 | [Val] Loss: 0.09126128256320953\n",
      "Epoch 566 -> [Train] Loss: 0.07556375861167908 | [Val] Loss: 0.09121594578027725\n",
      "Epoch 567 -> [Train] Loss: 0.07557222247123718 | [Val] Loss: 0.0913713201880455\n",
      "Epoch 568 -> [Train] Loss: 0.0755571648478508 | [Val] Loss: 0.09135479480028152\n",
      "Epoch 569 -> [Train] Loss: 0.07556045055389404 | [Val] Loss: 0.09120473265647888\n",
      "Epoch 570 -> [Train] Loss: 0.07553906738758087 | [Val] Loss: 0.09127461910247803\n",
      "Epoch 571 -> [Train] Loss: 0.07555921375751495 | [Val] Loss: 0.09136231988668442\n",
      "Epoch 572 -> [Train] Loss: 0.07552932947874069 | [Val] Loss: 0.0915154367685318\n",
      "Epoch 573 -> [Train] Loss: 0.07557907700538635 | [Val] Loss: 0.09120838344097137\n",
      "Epoch 574 -> [Train] Loss: 0.07554756850004196 | [Val] Loss: 0.09128852188587189\n",
      "Epoch 575 -> [Train] Loss: 0.07558323442935944 | [Val] Loss: 0.09128247201442719\n",
      "Epoch 576 -> [Train] Loss: 0.0755518451333046 | [Val] Loss: 0.09113346040248871\n",
      "Epoch 577 -> [Train] Loss: 0.07556402683258057 | [Val] Loss: 0.0914519727230072\n",
      "Epoch 578 -> [Train] Loss: 0.07559004426002502 | [Val] Loss: 0.09111203253269196\n",
      "Epoch 579 -> [Train] Loss: 0.07556220144033432 | [Val] Loss: 0.09138904511928558\n",
      "Epoch 580 -> [Train] Loss: 0.07556801289319992 | [Val] Loss: 0.0912424847483635\n",
      "Epoch 581 -> [Train] Loss: 0.07555549591779709 | [Val] Loss: 0.0911017656326294\n",
      "Epoch 582 -> [Train] Loss: 0.07555083930492401 | [Val] Loss: 0.0912400484085083\n",
      "Epoch 583 -> [Train] Loss: 0.0755622535943985 | [Val] Loss: 0.09114571660757065\n",
      "Epoch 584 -> [Train] Loss: 0.07556767761707306 | [Val] Loss: 0.09115403145551682\n",
      "Epoch 585 -> [Train] Loss: 0.07553233951330185 | [Val] Loss: 0.09139685332775116\n",
      "Epoch 586 -> [Train] Loss: 0.07555407285690308 | [Val] Loss: 0.09155299514532089\n",
      "Epoch 587 -> [Train] Loss: 0.07555422186851501 | [Val] Loss: 0.09101103991270065\n",
      "Epoch 588 -> [Train] Loss: 0.07555847615003586 | [Val] Loss: 0.09125810861587524\n",
      "Epoch 589 -> [Train] Loss: 0.07556529343128204 | [Val] Loss: 0.0910494476556778\n",
      "Epoch 590 -> [Train] Loss: 0.07556667923927307 | [Val] Loss: 0.09091459959745407\n",
      "Epoch 591 -> [Train] Loss: 0.07553546130657196 | [Val] Loss: 0.09108375012874603\n",
      "Epoch 592 -> [Train] Loss: 0.07552493363618851 | [Val] Loss: 0.09107339382171631\n",
      "Epoch 593 -> [Train] Loss: 0.07551699876785278 | [Val] Loss: 0.09143111854791641\n",
      "Epoch 594 -> [Train] Loss: 0.07553095370531082 | [Val] Loss: 0.09111269563436508\n",
      "Epoch 595 -> [Train] Loss: 0.07554933428764343 | [Val] Loss: 0.09122025966644287\n",
      "Epoch 596 -> [Train] Loss: 0.07554404437541962 | [Val] Loss: 0.09123779088258743\n",
      "Epoch 597 -> [Train] Loss: 0.07554106414318085 | [Val] Loss: 0.09118669480085373\n",
      "Epoch 598 -> [Train] Loss: 0.0755343958735466 | [Val] Loss: 0.09121531993150711\n",
      "Epoch 599 -> [Train] Loss: 0.07552990317344666 | [Val] Loss: 0.0912666916847229\n",
      "Epoch 600 -> [Train] Loss: 0.07553080469369888 | [Val] Loss: 0.09132319688796997\n",
      "Epoch 601 -> [Train] Loss: 0.07551286369562149 | [Val] Loss: 0.09127318114042282\n",
      "Epoch 602 -> [Train] Loss: 0.0755094587802887 | [Val] Loss: 0.09137731790542603\n",
      "Epoch 603 -> [Train] Loss: 0.07551772892475128 | [Val] Loss: 0.09159135073423386\n",
      "Epoch 604 -> [Train] Loss: 0.07552892714738846 | [Val] Loss: 0.09120757132768631\n",
      "Epoch 605 -> [Train] Loss: 0.0755009725689888 | [Val] Loss: 0.09146180003881454\n",
      "Epoch 606 -> [Train] Loss: 0.07550257444381714 | [Val] Loss: 0.09145469218492508\n",
      "Epoch 607 -> [Train] Loss: 0.0754876583814621 | [Val] Loss: 0.09134747087955475\n",
      "Epoch 608 -> [Train] Loss: 0.07549894601106644 | [Val] Loss: 0.0914091169834137\n",
      "Epoch 609 -> [Train] Loss: 0.07549691200256348 | [Val] Loss: 0.09123410284519196\n",
      "Epoch 610 -> [Train] Loss: 0.07549257576465607 | [Val] Loss: 0.09130506217479706\n",
      "Epoch 611 -> [Train] Loss: 0.07553067058324814 | [Val] Loss: 0.0913906842470169\n",
      "Epoch 612 -> [Train] Loss: 0.07547978311777115 | [Val] Loss: 0.09148679673671722\n",
      "Epoch 613 -> [Train] Loss: 0.07549343258142471 | [Val] Loss: 0.09122654050588608\n",
      "Epoch 614 -> [Train] Loss: 0.07549354434013367 | [Val] Loss: 0.09138701111078262\n",
      "Epoch 615 -> [Train] Loss: 0.0755167007446289 | [Val] Loss: 0.09116347879171371\n",
      "Epoch 616 -> [Train] Loss: 0.07549937069416046 | [Val] Loss: 0.09127373993396759\n",
      "Epoch 617 -> [Train] Loss: 0.07549893856048584 | [Val] Loss: 0.09135932475328445\n",
      "Epoch 618 -> [Train] Loss: 0.07548397034406662 | [Val] Loss: 0.09114157408475876\n",
      "Epoch 619 -> [Train] Loss: 0.07548147439956665 | [Val] Loss: 0.09130726754665375\n",
      "Epoch 620 -> [Train] Loss: 0.07548289746046066 | [Val] Loss: 0.09148485213518143\n",
      "Epoch 621 -> [Train] Loss: 0.0754731148481369 | [Val] Loss: 0.09142691642045975\n",
      "Epoch 622 -> [Train] Loss: 0.07549319416284561 | [Val] Loss: 0.09130030870437622\n",
      "Epoch 623 -> [Train] Loss: 0.07547897100448608 | [Val] Loss: 0.09139955788850784\n",
      "Epoch 624 -> [Train] Loss: 0.07547086477279663 | [Val] Loss: 0.09147334098815918\n",
      "Epoch 625 -> [Train] Loss: 0.0754590705037117 | [Val] Loss: 0.09138050675392151\n",
      "Epoch 626 -> [Train] Loss: 0.07548559457063675 | [Val] Loss: 0.09143105894327164\n",
      "Epoch 627 -> [Train] Loss: 0.0754387304186821 | [Val] Loss: 0.09124594181776047\n",
      "Epoch 628 -> [Train] Loss: 0.07546508312225342 | [Val] Loss: 0.0914202481508255\n",
      "Epoch 629 -> [Train] Loss: 0.07543289661407471 | [Val] Loss: 0.09151875972747803\n",
      "Epoch 630 -> [Train] Loss: 0.07544582337141037 | [Val] Loss: 0.09144067764282227\n",
      "Epoch 631 -> [Train] Loss: 0.07546631991863251 | [Val] Loss: 0.09147807955741882\n",
      "Epoch 632 -> [Train] Loss: 0.07545661181211472 | [Val] Loss: 0.09132348746061325\n",
      "Epoch 633 -> [Train] Loss: 0.07546326518058777 | [Val] Loss: 0.09152776747941971\n",
      "Epoch 634 -> [Train] Loss: 0.07543089985847473 | [Val] Loss: 0.09147066622972488\n",
      "Epoch 635 -> [Train] Loss: 0.07543714344501495 | [Val] Loss: 0.09159749746322632\n",
      "Epoch 636 -> [Train] Loss: 0.0754314661026001 | [Val] Loss: 0.09164734184741974\n",
      "Epoch 637 -> [Train] Loss: 0.07541657239198685 | [Val] Loss: 0.09173713624477386\n",
      "Epoch 638 -> [Train] Loss: 0.07544251531362534 | [Val] Loss: 0.0915154218673706\n",
      "Epoch 639 -> [Train] Loss: 0.07542335242033005 | [Val] Loss: 0.09146883338689804\n",
      "Epoch 640 -> [Train] Loss: 0.07543288916349411 | [Val] Loss: 0.09165681153535843\n",
      "Epoch 641 -> [Train] Loss: 0.07541726529598236 | [Val] Loss: 0.09127801656723022\n",
      "Epoch 642 -> [Train] Loss: 0.07540185749530792 | [Val] Loss: 0.09153614193201065\n",
      "Epoch 643 -> [Train] Loss: 0.07544579356908798 | [Val] Loss: 0.091661237180233\n",
      "Epoch 644 -> [Train] Loss: 0.07543101906776428 | [Val] Loss: 0.0914483293890953\n",
      "Epoch 645 -> [Train] Loss: 0.07539918273687363 | [Val] Loss: 0.09139968454837799\n",
      "Epoch 646 -> [Train] Loss: 0.07540298998355865 | [Val] Loss: 0.09152131527662277\n",
      "Epoch 647 -> [Train] Loss: 0.07539649307727814 | [Val] Loss: 0.09150245040655136\n",
      "Epoch 648 -> [Train] Loss: 0.0753961130976677 | [Val] Loss: 0.09147728979587555\n",
      "Epoch 649 -> [Train] Loss: 0.07539442181587219 | [Val] Loss: 0.09149254113435745\n",
      "Epoch 650 -> [Train] Loss: 0.07539347559213638 | [Val] Loss: 0.09153884649276733\n",
      "Epoch 651 -> [Train] Loss: 0.07538188993930817 | [Val] Loss: 0.09180784970521927\n",
      "Epoch 652 -> [Train] Loss: 0.07538414746522903 | [Val] Loss: 0.09162619709968567\n",
      "Epoch 653 -> [Train] Loss: 0.07539837062358856 | [Val] Loss: 0.09147583693265915\n",
      "Epoch 654 -> [Train] Loss: 0.07538525760173798 | [Val] Loss: 0.09156061708927155\n",
      "Epoch 655 -> [Train] Loss: 0.07539105415344238 | [Val] Loss: 0.0914536565542221\n",
      "Epoch 656 -> [Train] Loss: 0.0753932073712349 | [Val] Loss: 0.09160741418600082\n",
      "Epoch 657 -> [Train] Loss: 0.07538712024688721 | [Val] Loss: 0.09152553975582123\n",
      "Epoch 658 -> [Train] Loss: 0.07536860555410385 | [Val] Loss: 0.09150701761245728\n",
      "Epoch 659 -> [Train] Loss: 0.07538443803787231 | [Val] Loss: 0.09169643372297287\n",
      "Epoch 660 -> [Train] Loss: 0.07535873353481293 | [Val] Loss: 0.09166014939546585\n",
      "Epoch 661 -> [Train] Loss: 0.07534638792276382 | [Val] Loss: 0.09159774333238602\n",
      "Epoch 662 -> [Train] Loss: 0.07534116506576538 | [Val] Loss: 0.09179452806711197\n",
      "Epoch 663 -> [Train] Loss: 0.07535434514284134 | [Val] Loss: 0.09173917025327682\n",
      "Epoch 664 -> [Train] Loss: 0.07534157484769821 | [Val] Loss: 0.09143472462892532\n",
      "Epoch 665 -> [Train] Loss: 0.07534738630056381 | [Val] Loss: 0.09167766571044922\n",
      "Epoch 666 -> [Train] Loss: 0.07532820105552673 | [Val] Loss: 0.09163039922714233\n",
      "Epoch 667 -> [Train] Loss: 0.07536223530769348 | [Val] Loss: 0.09159041941165924\n",
      "Epoch 668 -> [Train] Loss: 0.07534962147474289 | [Val] Loss: 0.0918060764670372\n",
      "Epoch 669 -> [Train] Loss: 0.07532227039337158 | [Val] Loss: 0.0916404277086258\n",
      "Epoch 670 -> [Train] Loss: 0.07534653693437576 | [Val] Loss: 0.09183280915021896\n",
      "Epoch 671 -> [Train] Loss: 0.07536306977272034 | [Val] Loss: 0.09186211228370667\n",
      "Epoch 672 -> [Train] Loss: 0.07532265782356262 | [Val] Loss: 0.09162595868110657\n",
      "Epoch 673 -> [Train] Loss: 0.07534212619066238 | [Val] Loss: 0.09169349074363708\n",
      "Epoch 674 -> [Train] Loss: 0.07537829130887985 | [Val] Loss: 0.09185361117124557\n",
      "Epoch 675 -> [Train] Loss: 0.07531416416168213 | [Val] Loss: 0.09185188263654709\n",
      "Epoch 676 -> [Train] Loss: 0.07531756162643433 | [Val] Loss: 0.09200141578912735\n",
      "Epoch 677 -> [Train] Loss: 0.07531265169382095 | [Val] Loss: 0.0917721763253212\n",
      "Epoch 678 -> [Train] Loss: 0.07532144337892532 | [Val] Loss: 0.0918096974492073\n",
      "Epoch 679 -> [Train] Loss: 0.07528429478406906 | [Val] Loss: 0.09181248396635056\n",
      "Epoch 680 -> [Train] Loss: 0.07529795169830322 | [Val] Loss: 0.09192337095737457\n",
      "Epoch 681 -> [Train] Loss: 0.07530353963375092 | [Val] Loss: 0.09182491898536682\n",
      "Epoch 682 -> [Train] Loss: 0.07532379031181335 | [Val] Loss: 0.09190699458122253\n",
      "Epoch 683 -> [Train] Loss: 0.07528991997241974 | [Val] Loss: 0.09169071167707443\n",
      "Epoch 684 -> [Train] Loss: 0.07530798763036728 | [Val] Loss: 0.09194818884134293\n",
      "Epoch 685 -> [Train] Loss: 0.07527507096529007 | [Val] Loss: 0.09192905575037003\n",
      "Epoch 686 -> [Train] Loss: 0.07527937740087509 | [Val] Loss: 0.09181783348321915\n",
      "Epoch 687 -> [Train] Loss: 0.07530958950519562 | [Val] Loss: 0.09201939404010773\n",
      "Epoch 688 -> [Train] Loss: 0.07527285814285278 | [Val] Loss: 0.09178616851568222\n",
      "Epoch 689 -> [Train] Loss: 0.07527226209640503 | [Val] Loss: 0.09174725413322449\n",
      "Epoch 690 -> [Train] Loss: 0.07524915784597397 | [Val] Loss: 0.09198398143053055\n",
      "Epoch 691 -> [Train] Loss: 0.07529676705598831 | [Val] Loss: 0.09199871122837067\n",
      "Epoch 692 -> [Train] Loss: 0.07528740167617798 | [Val] Loss: 0.09199237823486328\n",
      "Epoch 693 -> [Train] Loss: 0.07529245316982269 | [Val] Loss: 0.09206827729940414\n",
      "Epoch 694 -> [Train] Loss: 0.075261689722538 | [Val] Loss: 0.09197067469358444\n",
      "Epoch 695 -> [Train] Loss: 0.075287364423275 | [Val] Loss: 0.09203112870454788\n",
      "Epoch 696 -> [Train] Loss: 0.07527078688144684 | [Val] Loss: 0.09191436320543289\n",
      "Epoch 697 -> [Train] Loss: 0.07526451349258423 | [Val] Loss: 0.09199102222919464\n",
      "Epoch 698 -> [Train] Loss: 0.0752766951918602 | [Val] Loss: 0.09185782819986343\n",
      "Epoch 699 -> [Train] Loss: 0.07523074001073837 | [Val] Loss: 0.09182168543338776\n",
      "Epoch 700 -> [Train] Loss: 0.07526077330112457 | [Val] Loss: 0.09194988012313843\n",
      "Epoch 701 -> [Train] Loss: 0.07525940984487534 | [Val] Loss: 0.09200818836688995\n",
      "Epoch 702 -> [Train] Loss: 0.07524756342172623 | [Val] Loss: 0.09200803190469742\n",
      "Epoch 703 -> [Train] Loss: 0.07526414841413498 | [Val] Loss: 0.09171351790428162\n",
      "Epoch 704 -> [Train] Loss: 0.07525182515382767 | [Val] Loss: 0.09188256412744522\n",
      "Epoch 705 -> [Train] Loss: 0.07524650543928146 | [Val] Loss: 0.09198649227619171\n",
      "Epoch 706 -> [Train] Loss: 0.07524364441633224 | [Val] Loss: 0.09209132939577103\n",
      "Epoch 707 -> [Train] Loss: 0.0752391368150711 | [Val] Loss: 0.09214144200086594\n",
      "Epoch 708 -> [Train] Loss: 0.07524929195642471 | [Val] Loss: 0.09204626828432083\n",
      "Epoch 709 -> [Train] Loss: 0.07525037974119186 | [Val] Loss: 0.09199733287096024\n",
      "Epoch 710 -> [Train] Loss: 0.0752561017870903 | [Val] Loss: 0.09205181151628494\n",
      "Epoch 711 -> [Train] Loss: 0.07523860037326813 | [Val] Loss: 0.09221100807189941\n",
      "Epoch 712 -> [Train] Loss: 0.07523620873689651 | [Val] Loss: 0.09204733371734619\n",
      "Epoch 713 -> [Train] Loss: 0.07522337883710861 | [Val] Loss: 0.09190362691879272\n",
      "Epoch 714 -> [Train] Loss: 0.07521333545446396 | [Val] Loss: 0.09230166673660278\n",
      "Epoch 715 -> [Train] Loss: 0.07522731274366379 | [Val] Loss: 0.09199616312980652\n",
      "Epoch 716 -> [Train] Loss: 0.07521418482065201 | [Val] Loss: 0.09194373339414597\n",
      "Epoch 717 -> [Train] Loss: 0.07521209120750427 | [Val] Loss: 0.09219881892204285\n",
      "Epoch 718 -> [Train] Loss: 0.07522249221801758 | [Val] Loss: 0.0921020358800888\n",
      "Epoch 719 -> [Train] Loss: 0.07520092278718948 | [Val] Loss: 0.0921785980463028\n",
      "Epoch 720 -> [Train] Loss: 0.07519328594207764 | [Val] Loss: 0.09214285016059875\n",
      "Epoch 721 -> [Train] Loss: 0.07519511878490448 | [Val] Loss: 0.09223690629005432\n",
      "Epoch 722 -> [Train] Loss: 0.0751849040389061 | [Val] Loss: 0.09210342913866043\n",
      "Epoch 723 -> [Train] Loss: 0.07519873976707458 | [Val] Loss: 0.09220895916223526\n",
      "Epoch 724 -> [Train] Loss: 0.0751870796084404 | [Val] Loss: 0.09198366105556488\n",
      "Epoch 725 -> [Train] Loss: 0.07520519196987152 | [Val] Loss: 0.09195806831121445\n",
      "Epoch 726 -> [Train] Loss: 0.0751921758055687 | [Val] Loss: 0.09211639314889908\n",
      "Epoch 727 -> [Train] Loss: 0.07520271837711334 | [Val] Loss: 0.09209241718053818\n",
      "Epoch 728 -> [Train] Loss: 0.07518211007118225 | [Val] Loss: 0.09220948070287704\n",
      "Epoch 729 -> [Train] Loss: 0.07519406080245972 | [Val] Loss: 0.09199973195791245\n",
      "Epoch 730 -> [Train] Loss: 0.07516668736934662 | [Val] Loss: 0.09230883419513702\n",
      "Epoch 731 -> [Train] Loss: 0.07516255974769592 | [Val] Loss: 0.09242194890975952\n",
      "Epoch 732 -> [Train] Loss: 0.07515764981508255 | [Val] Loss: 0.09233491122722626\n",
      "Epoch 733 -> [Train] Loss: 0.07516027987003326 | [Val] Loss: 0.09245968610048294\n",
      "Epoch 734 -> [Train] Loss: 0.07519816607236862 | [Val] Loss: 0.09220273792743683\n",
      "Epoch 735 -> [Train] Loss: 0.07518380880355835 | [Val] Loss: 0.09200727194547653\n",
      "Epoch 736 -> [Train] Loss: 0.07515423744916916 | [Val] Loss: 0.09236147254705429\n",
      "Epoch 737 -> [Train] Loss: 0.07518138736486435 | [Val] Loss: 0.09207434207201004\n",
      "Epoch 738 -> [Train] Loss: 0.07514781504869461 | [Val] Loss: 0.09243492782115936\n",
      "Epoch 739 -> [Train] Loss: 0.07515764981508255 | [Val] Loss: 0.09245463460683823\n",
      "Epoch 740 -> [Train] Loss: 0.0751323327422142 | [Val] Loss: 0.09240534901618958\n",
      "Epoch 741 -> [Train] Loss: 0.0751510038971901 | [Val] Loss: 0.0923851802945137\n",
      "Epoch 742 -> [Train] Loss: 0.07515526562929153 | [Val] Loss: 0.09246022254228592\n",
      "Epoch 743 -> [Train] Loss: 0.07514044642448425 | [Val] Loss: 0.09245186299085617\n",
      "Epoch 744 -> [Train] Loss: 0.07513592392206192 | [Val] Loss: 0.09266635030508041\n",
      "Epoch 745 -> [Train] Loss: 0.07512801140546799 | [Val] Loss: 0.09252484887838364\n",
      "Epoch 746 -> [Train] Loss: 0.07512291520833969 | [Val] Loss: 0.09228020906448364\n",
      "Epoch 747 -> [Train] Loss: 0.07512030750513077 | [Val] Loss: 0.09260573983192444\n",
      "Epoch 748 -> [Train] Loss: 0.07514216750860214 | [Val] Loss: 0.09248113632202148\n",
      "Epoch 749 -> [Train] Loss: 0.07512958347797394 | [Val] Loss: 0.09259684383869171\n",
      "Epoch 750 -> [Train] Loss: 0.07511675357818604 | [Val] Loss: 0.09245803952217102\n",
      "Epoch 751 -> [Train] Loss: 0.07509628683328629 | [Val] Loss: 0.09233054518699646\n",
      "Epoch 752 -> [Train] Loss: 0.07512474805116653 | [Val] Loss: 0.09227436780929565\n",
      "Epoch 753 -> [Train] Loss: 0.07512081414461136 | [Val] Loss: 0.09251387417316437\n",
      "Epoch 754 -> [Train] Loss: 0.07513175159692764 | [Val] Loss: 0.09233180433511734\n",
      "Epoch 755 -> [Train] Loss: 0.07509235292673111 | [Val] Loss: 0.09246405959129333\n",
      "Epoch 756 -> [Train] Loss: 0.07510515302419662 | [Val] Loss: 0.09250187873840332\n",
      "Epoch 757 -> [Train] Loss: 0.07508789002895355 | [Val] Loss: 0.09260443598031998\n",
      "Epoch 758 -> [Train] Loss: 0.07508135586977005 | [Val] Loss: 0.09256766736507416\n",
      "Epoch 759 -> [Train] Loss: 0.07509421557188034 | [Val] Loss: 0.09265976399183273\n",
      "Epoch 760 -> [Train] Loss: 0.07510446757078171 | [Val] Loss: 0.09225638955831528\n",
      "Epoch 761 -> [Train] Loss: 0.07510486990213394 | [Val] Loss: 0.09229544550180435\n",
      "Epoch 762 -> [Train] Loss: 0.07507757097482681 | [Val] Loss: 0.09238627552986145\n",
      "Epoch 763 -> [Train] Loss: 0.07509308308362961 | [Val] Loss: 0.0922582820057869\n",
      "Epoch 764 -> [Train] Loss: 0.075130894780159 | [Val] Loss: 0.09222845733165741\n",
      "Epoch 765 -> [Train] Loss: 0.0750681459903717 | [Val] Loss: 0.09238344430923462\n",
      "Epoch 766 -> [Train] Loss: 0.0750894844532013 | [Val] Loss: 0.09247664362192154\n",
      "Epoch 767 -> [Train] Loss: 0.07508649677038193 | [Val] Loss: 0.09233395755290985\n",
      "Epoch 768 -> [Train] Loss: 0.07509227842092514 | [Val] Loss: 0.09221258759498596\n",
      "Epoch 769 -> [Train] Loss: 0.07505155354738235 | [Val] Loss: 0.09254082292318344\n",
      "Epoch 770 -> [Train] Loss: 0.07505982369184494 | [Val] Loss: 0.09233295172452927\n",
      "Epoch 771 -> [Train] Loss: 0.07506358623504639 | [Val] Loss: 0.09225433319807053\n",
      "Epoch 772 -> [Train] Loss: 0.07507764548063278 | [Val] Loss: 0.09236887842416763\n",
      "Epoch 773 -> [Train] Loss: 0.0750821903347969 | [Val] Loss: 0.09253942221403122\n",
      "Epoch 774 -> [Train] Loss: 0.07505010813474655 | [Val] Loss: 0.09229204803705215\n",
      "Epoch 775 -> [Train] Loss: 0.07503169029951096 | [Val] Loss: 0.09251785278320312\n",
      "Epoch 776 -> [Train] Loss: 0.07506629824638367 | [Val] Loss: 0.09233158081769943\n",
      "Epoch 777 -> [Train] Loss: 0.07508354634046555 | [Val] Loss: 0.09240640699863434\n",
      "Epoch 778 -> [Train] Loss: 0.07505987584590912 | [Val] Loss: 0.09257742762565613\n",
      "Epoch 779 -> [Train] Loss: 0.07503960281610489 | [Val] Loss: 0.09240537881851196\n",
      "Epoch 780 -> [Train] Loss: 0.0750473216176033 | [Val] Loss: 0.09260236471891403\n",
      "Epoch 781 -> [Train] Loss: 0.07505711168050766 | [Val] Loss: 0.09239665418863297\n",
      "Epoch 782 -> [Train] Loss: 0.07503274083137512 | [Val] Loss: 0.09239456057548523\n",
      "Epoch 783 -> [Train] Loss: 0.07505445182323456 | [Val] Loss: 0.09250040352344513\n",
      "Epoch 784 -> [Train] Loss: 0.07503612339496613 | [Val] Loss: 0.09246637672185898\n",
      "Epoch 785 -> [Train] Loss: 0.07501940429210663 | [Val] Loss: 0.09251721203327179\n",
      "Epoch 786 -> [Train] Loss: 0.07501105219125748 | [Val] Loss: 0.09214670956134796\n",
      "Epoch 787 -> [Train] Loss: 0.07503511011600494 | [Val] Loss: 0.0924205631017685\n",
      "Epoch 788 -> [Train] Loss: 0.07503725588321686 | [Val] Loss: 0.09235913306474686\n",
      "Epoch 789 -> [Train] Loss: 0.0750197321176529 | [Val] Loss: 0.09212205559015274\n",
      "Epoch 790 -> [Train] Loss: 0.07503752410411835 | [Val] Loss: 0.09242177754640579\n",
      "Epoch 791 -> [Train] Loss: 0.07503285259008408 | [Val] Loss: 0.09228527545928955\n",
      "Epoch 792 -> [Train] Loss: 0.07501918077468872 | [Val] Loss: 0.0921279564499855\n",
      "Epoch 793 -> [Train] Loss: 0.07501920312643051 | [Val] Loss: 0.09224292635917664\n",
      "Epoch 794 -> [Train] Loss: 0.07501951605081558 | [Val] Loss: 0.09219694882631302\n",
      "Epoch 795 -> [Train] Loss: 0.0750294178724289 | [Val] Loss: 0.09248100966215134\n",
      "Epoch 796 -> [Train] Loss: 0.07501735538244247 | [Val] Loss: 0.09251096099615097\n",
      "Epoch 797 -> [Train] Loss: 0.07501784712076187 | [Val] Loss: 0.09241693466901779\n",
      "Epoch 798 -> [Train] Loss: 0.07501406222581863 | [Val] Loss: 0.09244407713413239\n",
      "Epoch 799 -> [Train] Loss: 0.07500728219747543 | [Val] Loss: 0.09250019490718842\n",
      "Epoch 800 -> [Train] Loss: 0.07500290870666504 | [Val] Loss: 0.09244125336408615\n",
      "Epoch 801 -> [Train] Loss: 0.07503128051757812 | [Val] Loss: 0.09243141859769821\n",
      "Epoch 802 -> [Train] Loss: 0.07503261417150497 | [Val] Loss: 0.0923188105225563\n",
      "Epoch 803 -> [Train] Loss: 0.07500918954610825 | [Val] Loss: 0.09236210584640503\n",
      "Epoch 804 -> [Train] Loss: 0.07498946785926819 | [Val] Loss: 0.09259752184152603\n",
      "Epoch 805 -> [Train] Loss: 0.07500157505273819 | [Val] Loss: 0.09256294369697571\n",
      "Epoch 806 -> [Train] Loss: 0.07497677206993103 | [Val] Loss: 0.09265945851802826\n",
      "Epoch 807 -> [Train] Loss: 0.07500608265399933 | [Val] Loss: 0.09233450889587402\n",
      "Epoch 808 -> [Train] Loss: 0.0750015527009964 | [Val] Loss: 0.09227713942527771\n",
      "Epoch 809 -> [Train] Loss: 0.07502735406160355 | [Val] Loss: 0.09264327585697174\n",
      "Epoch 810 -> [Train] Loss: 0.07501154392957687 | [Val] Loss: 0.09240011125802994\n",
      "Epoch 811 -> [Train] Loss: 0.07500127702951431 | [Val] Loss: 0.09251506626605988\n",
      "Epoch 812 -> [Train] Loss: 0.07499083876609802 | [Val] Loss: 0.09254759550094604\n",
      "Epoch 813 -> [Train] Loss: 0.07500015944242477 | [Val] Loss: 0.09266042709350586\n",
      "Epoch 814 -> [Train] Loss: 0.07496722042560577 | [Val] Loss: 0.09267571568489075\n",
      "Epoch 815 -> [Train] Loss: 0.07499191164970398 | [Val] Loss: 0.09249984472990036\n",
      "Epoch 816 -> [Train] Loss: 0.07500268518924713 | [Val] Loss: 0.09245403110980988\n",
      "Epoch 817 -> [Train] Loss: 0.075008824467659 | [Val] Loss: 0.09208379685878754\n",
      "Epoch 818 -> [Train] Loss: 0.07499026507139206 | [Val] Loss: 0.09235770255327225\n",
      "Epoch 819 -> [Train] Loss: 0.07495672255754471 | [Val] Loss: 0.09241830557584763\n",
      "Epoch 820 -> [Train] Loss: 0.07496415823698044 | [Val] Loss: 0.0924951359629631\n",
      "Epoch 821 -> [Train] Loss: 0.07498889416456223 | [Val] Loss: 0.09243213385343552\n",
      "Epoch 822 -> [Train] Loss: 0.07497335225343704 | [Val] Loss: 0.09254679083824158\n",
      "Epoch 823 -> [Train] Loss: 0.07498981058597565 | [Val] Loss: 0.09246956557035446\n",
      "Epoch 824 -> [Train] Loss: 0.07495490461587906 | [Val] Loss: 0.09263700246810913\n",
      "Epoch 825 -> [Train] Loss: 0.074955515563488 | [Val] Loss: 0.0925113633275032\n",
      "Epoch 826 -> [Train] Loss: 0.0749673917889595 | [Val] Loss: 0.09269991517066956\n",
      "Epoch 827 -> [Train] Loss: 0.07497008144855499 | [Val] Loss: 0.09266369044780731\n",
      "Epoch 828 -> [Train] Loss: 0.0749574825167656 | [Val] Loss: 0.09254168719053268\n",
      "Epoch 829 -> [Train] Loss: 0.07492858916521072 | [Val] Loss: 0.09266726672649384\n",
      "Epoch 830 -> [Train] Loss: 0.07495035231113434 | [Val] Loss: 0.09273677319288254\n",
      "Epoch 831 -> [Train] Loss: 0.07495641708374023 | [Val] Loss: 0.09280034899711609\n",
      "Epoch 832 -> [Train] Loss: 0.07495415955781937 | [Val] Loss: 0.09254120290279388\n",
      "Epoch 833 -> [Train] Loss: 0.0749480128288269 | [Val] Loss: 0.09266050159931183\n",
      "Epoch 834 -> [Train] Loss: 0.0749397948384285 | [Val] Loss: 0.09282660484313965\n",
      "Epoch 835 -> [Train] Loss: 0.07495401799678802 | [Val] Loss: 0.09251575171947479\n",
      "Epoch 836 -> [Train] Loss: 0.0749354287981987 | [Val] Loss: 0.0926542654633522\n",
      "Epoch 837 -> [Train] Loss: 0.07496197521686554 | [Val] Loss: 0.09266062080860138\n",
      "Epoch 838 -> [Train] Loss: 0.0749402791261673 | [Val] Loss: 0.0928010493516922\n",
      "Epoch 839 -> [Train] Loss: 0.07493268698453903 | [Val] Loss: 0.0927722156047821\n",
      "Epoch 840 -> [Train] Loss: 0.07494033128023148 | [Val] Loss: 0.09272005409002304\n",
      "Epoch 841 -> [Train] Loss: 0.07492666691541672 | [Val] Loss: 0.0929829478263855\n",
      "Epoch 842 -> [Train] Loss: 0.07494142651557922 | [Val] Loss: 0.09291085600852966\n",
      "Epoch 843 -> [Train] Loss: 0.07495841383934021 | [Val] Loss: 0.09302866458892822\n",
      "Epoch 844 -> [Train] Loss: 0.0749424546957016 | [Val] Loss: 0.09286176413297653\n",
      "Epoch 845 -> [Train] Loss: 0.0749649703502655 | [Val] Loss: 0.0928412526845932\n",
      "Epoch 846 -> [Train] Loss: 0.07492831349372864 | [Val] Loss: 0.09280446171760559\n",
      "Epoch 847 -> [Train] Loss: 0.07490968704223633 | [Val] Loss: 0.09271997213363647\n",
      "Epoch 848 -> [Train] Loss: 0.07492759078741074 | [Val] Loss: 0.09271270781755447\n",
      "Epoch 849 -> [Train] Loss: 0.07490426301956177 | [Val] Loss: 0.09272860735654831\n",
      "Epoch 850 -> [Train] Loss: 0.07492097467184067 | [Val] Loss: 0.09301848709583282\n",
      "Epoch 851 -> [Train] Loss: 0.07490898668766022 | [Val] Loss: 0.09293986111879349\n",
      "Epoch 852 -> [Train] Loss: 0.07492288947105408 | [Val] Loss: 0.09278951585292816\n",
      "Epoch 853 -> [Train] Loss: 0.07489773631095886 | [Val] Loss: 0.09299645572900772\n",
      "Epoch 854 -> [Train] Loss: 0.07490106672048569 | [Val] Loss: 0.09287762641906738\n",
      "Epoch 855 -> [Train] Loss: 0.07490662485361099 | [Val] Loss: 0.09283096343278885\n",
      "Epoch 856 -> [Train] Loss: 0.07491177320480347 | [Val] Loss: 0.0928775742650032\n",
      "Epoch 857 -> [Train] Loss: 0.07490431517362595 | [Val] Loss: 0.09293446689844131\n",
      "Epoch 858 -> [Train] Loss: 0.07488520443439484 | [Val] Loss: 0.09308092296123505\n",
      "Epoch 859 -> [Train] Loss: 0.07490585744380951 | [Val] Loss: 0.09305819123983383\n",
      "Epoch 860 -> [Train] Loss: 0.07490594685077667 | [Val] Loss: 0.09324368834495544\n",
      "Epoch 861 -> [Train] Loss: 0.07489319145679474 | [Val] Loss: 0.09301701933145523\n",
      "Epoch 862 -> [Train] Loss: 0.07489251345396042 | [Val] Loss: 0.09308070689439774\n",
      "Epoch 863 -> [Train] Loss: 0.07491813600063324 | [Val] Loss: 0.09314746409654617\n",
      "Epoch 864 -> [Train] Loss: 0.07488339394330978 | [Val] Loss: 0.09300288558006287\n",
      "Epoch 865 -> [Train] Loss: 0.0748927965760231 | [Val] Loss: 0.09295683354139328\n",
      "Epoch 866 -> [Train] Loss: 0.07488379627466202 | [Val] Loss: 0.0929756611585617\n",
      "Epoch 867 -> [Train] Loss: 0.0748867392539978 | [Val] Loss: 0.09292092174291611\n",
      "Epoch 868 -> [Train] Loss: 0.07488664239645004 | [Val] Loss: 0.09288126230239868\n",
      "Epoch 869 -> [Train] Loss: 0.07487855106592178 | [Val] Loss: 0.09308803081512451\n",
      "Epoch 870 -> [Train] Loss: 0.07490190118551254 | [Val] Loss: 0.09288152307271957\n",
      "Epoch 871 -> [Train] Loss: 0.07487921416759491 | [Val] Loss: 0.09291505068540573\n",
      "Epoch 872 -> [Train] Loss: 0.074891597032547 | [Val] Loss: 0.09269549697637558\n",
      "Epoch 873 -> [Train] Loss: 0.0748964250087738 | [Val] Loss: 0.09283094853162766\n",
      "Epoch 874 -> [Train] Loss: 0.07486167550086975 | [Val] Loss: 0.0929572582244873\n",
      "Epoch 875 -> [Train] Loss: 0.07488816231489182 | [Val] Loss: 0.09296358376741409\n",
      "Epoch 876 -> [Train] Loss: 0.07485580444335938 | [Val] Loss: 0.09298449754714966\n",
      "Epoch 877 -> [Train] Loss: 0.07485602796077728 | [Val] Loss: 0.09313560277223587\n",
      "Epoch 878 -> [Train] Loss: 0.0748579353094101 | [Val] Loss: 0.09316552430391312\n",
      "Epoch 879 -> [Train] Loss: 0.0748768001794815 | [Val] Loss: 0.0931469053030014\n",
      "Epoch 880 -> [Train] Loss: 0.07488840818405151 | [Val] Loss: 0.09298539161682129\n",
      "Epoch 881 -> [Train] Loss: 0.07486279308795929 | [Val] Loss: 0.09322384744882584\n",
      "Epoch 882 -> [Train] Loss: 0.07485594600439072 | [Val] Loss: 0.0930948555469513\n",
      "Epoch 883 -> [Train] Loss: 0.0748658999800682 | [Val] Loss: 0.09295789152383804\n",
      "Epoch 884 -> [Train] Loss: 0.0748797208070755 | [Val] Loss: 0.09307920187711716\n",
      "Epoch 885 -> [Train] Loss: 0.0748521164059639 | [Val] Loss: 0.09319277852773666\n",
      "Epoch 886 -> [Train] Loss: 0.07483725994825363 | [Val] Loss: 0.09315225481987\n",
      "Epoch 887 -> [Train] Loss: 0.07485318183898926 | [Val] Loss: 0.0929328054189682\n",
      "Epoch 888 -> [Train] Loss: 0.07487329840660095 | [Val] Loss: 0.0931285172700882\n",
      "Epoch 889 -> [Train] Loss: 0.07485552132129669 | [Val] Loss: 0.09332286566495895\n",
      "Epoch 890 -> [Train] Loss: 0.074847511947155 | [Val] Loss: 0.0931406244635582\n",
      "Epoch 891 -> [Train] Loss: 0.07484132051467896 | [Val] Loss: 0.09311579912900925\n",
      "Epoch 892 -> [Train] Loss: 0.07484693080186844 | [Val] Loss: 0.09338387846946716\n",
      "Epoch 893 -> [Train] Loss: 0.07483135908842087 | [Val] Loss: 0.0930992066860199\n",
      "Epoch 894 -> [Train] Loss: 0.07485169917345047 | [Val] Loss: 0.09313539415597916\n",
      "Epoch 895 -> [Train] Loss: 0.07485471665859222 | [Val] Loss: 0.0931948721408844\n",
      "Epoch 896 -> [Train] Loss: 0.07482967525720596 | [Val] Loss: 0.09307803213596344\n",
      "Epoch 897 -> [Train] Loss: 0.07483995705842972 | [Val] Loss: 0.09326961636543274\n",
      "Epoch 898 -> [Train] Loss: 0.07482525706291199 | [Val] Loss: 0.09319192171096802\n",
      "Epoch 899 -> [Train] Loss: 0.07482358068227768 | [Val] Loss: 0.09329574555158615\n",
      "Epoch 900 -> [Train] Loss: 0.07483360916376114 | [Val] Loss: 0.09357567131519318\n",
      "Epoch 901 -> [Train] Loss: 0.07484865188598633 | [Val] Loss: 0.09327446669340134\n",
      "Epoch 902 -> [Train] Loss: 0.07481976598501205 | [Val] Loss: 0.093362957239151\n",
      "Epoch 903 -> [Train] Loss: 0.07482364773750305 | [Val] Loss: 0.09339550137519836\n",
      "Epoch 904 -> [Train] Loss: 0.07482524961233139 | [Val] Loss: 0.09332948178052902\n",
      "Epoch 905 -> [Train] Loss: 0.07481708377599716 | [Val] Loss: 0.09351906925439835\n",
      "Epoch 906 -> [Train] Loss: 0.07481563091278076 | [Val] Loss: 0.09327533841133118\n",
      "Epoch 907 -> [Train] Loss: 0.07480241358280182 | [Val] Loss: 0.09332127124071121\n",
      "Epoch 908 -> [Train] Loss: 0.07482808083295822 | [Val] Loss: 0.09334435313940048\n",
      "Epoch 909 -> [Train] Loss: 0.07480545341968536 | [Val] Loss: 0.09324990957975388\n",
      "Epoch 910 -> [Train] Loss: 0.07479856163263321 | [Val] Loss: 0.09329349547624588\n",
      "Epoch 911 -> [Train] Loss: 0.07480869442224503 | [Val] Loss: 0.09336845576763153\n",
      "Epoch 912 -> [Train] Loss: 0.07479573041200638 | [Val] Loss: 0.09340860694646835\n",
      "Epoch 913 -> [Train] Loss: 0.07481467723846436 | [Val] Loss: 0.09333838522434235\n",
      "Epoch 914 -> [Train] Loss: 0.07482384145259857 | [Val] Loss: 0.09329695999622345\n",
      "Epoch 915 -> [Train] Loss: 0.07480712980031967 | [Val] Loss: 0.09334015846252441\n",
      "Epoch 916 -> [Train] Loss: 0.07479652017354965 | [Val] Loss: 0.09341058135032654\n",
      "Epoch 917 -> [Train] Loss: 0.07479343563318253 | [Val] Loss: 0.09349405020475388\n",
      "Epoch 918 -> [Train] Loss: 0.07477167993783951 | [Val] Loss: 0.09340988099575043\n",
      "Epoch 919 -> [Train] Loss: 0.0747886598110199 | [Val] Loss: 0.09367649257183075\n",
      "Epoch 920 -> [Train] Loss: 0.07480404525995255 | [Val] Loss: 0.09325744956731796\n",
      "Epoch 921 -> [Train] Loss: 0.07478010654449463 | [Val] Loss: 0.09309294074773788\n",
      "Epoch 922 -> [Train] Loss: 0.0747864693403244 | [Val] Loss: 0.09325836598873138\n",
      "Epoch 923 -> [Train] Loss: 0.0747491642832756 | [Val] Loss: 0.09342023730278015\n",
      "Epoch 924 -> [Train] Loss: 0.07478220760822296 | [Val] Loss: 0.09323453903198242\n",
      "Epoch 925 -> [Train] Loss: 0.07476291805505753 | [Val] Loss: 0.09336032718420029\n",
      "Epoch 926 -> [Train] Loss: 0.07476726174354553 | [Val] Loss: 0.093043252825737\n",
      "Epoch 927 -> [Train] Loss: 0.07476812601089478 | [Val] Loss: 0.09339787811040878\n",
      "Epoch 928 -> [Train] Loss: 0.07476820796728134 | [Val] Loss: 0.09313294291496277\n",
      "Epoch 929 -> [Train] Loss: 0.07474830001592636 | [Val] Loss: 0.09351202100515366\n",
      "Epoch 930 -> [Train] Loss: 0.07477626204490662 | [Val] Loss: 0.09321799874305725\n",
      "Epoch 931 -> [Train] Loss: 0.0747469961643219 | [Val] Loss: 0.09318806231021881\n",
      "Epoch 932 -> [Train] Loss: 0.07477991282939911 | [Val] Loss: 0.09316324442625046\n",
      "Epoch 933 -> [Train] Loss: 0.0747750848531723 | [Val] Loss: 0.09308396279811859\n",
      "Epoch 934 -> [Train] Loss: 0.07476672530174255 | [Val] Loss: 0.09318321198225021\n",
      "Epoch 935 -> [Train] Loss: 0.07479091733694077 | [Val] Loss: 0.09331715852022171\n",
      "Epoch 936 -> [Train] Loss: 0.07476107776165009 | [Val] Loss: 0.09323599189519882\n",
      "Epoch 937 -> [Train] Loss: 0.07475361227989197 | [Val] Loss: 0.09325587004423141\n",
      "Epoch 938 -> [Train] Loss: 0.07474994659423828 | [Val] Loss: 0.09337220340967178\n",
      "Epoch 939 -> [Train] Loss: 0.07475799322128296 | [Val] Loss: 0.09359821677207947\n",
      "Epoch 940 -> [Train] Loss: 0.07475783675909042 | [Val] Loss: 0.09334950149059296\n",
      "Epoch 941 -> [Train] Loss: 0.07474041730165482 | [Val] Loss: 0.09346293658018112\n",
      "Epoch 942 -> [Train] Loss: 0.07476449757814407 | [Val] Loss: 0.09322722256183624\n",
      "Epoch 943 -> [Train] Loss: 0.07475049048662186 | [Val] Loss: 0.09351449459791183\n",
      "Epoch 944 -> [Train] Loss: 0.0747503787279129 | [Val] Loss: 0.09332634508609772\n",
      "Epoch 945 -> [Train] Loss: 0.07476267218589783 | [Val] Loss: 0.09339312463998795\n",
      "Epoch 946 -> [Train] Loss: 0.07474314421415329 | [Val] Loss: 0.0933501347899437\n",
      "Epoch 947 -> [Train] Loss: 0.07473764568567276 | [Val] Loss: 0.0933777391910553\n",
      "Epoch 948 -> [Train] Loss: 0.07472479343414307 | [Val] Loss: 0.09350645542144775\n",
      "Epoch 949 -> [Train] Loss: 0.07475319504737854 | [Val] Loss: 0.09333395957946777\n",
      "Epoch 950 -> [Train] Loss: 0.07473146170377731 | [Val] Loss: 0.09334979206323624\n",
      "Epoch 951 -> [Train] Loss: 0.07472779601812363 | [Val] Loss: 0.0936957448720932\n",
      "Epoch 952 -> [Train] Loss: 0.07472272962331772 | [Val] Loss: 0.093492791056633\n",
      "Epoch 953 -> [Train] Loss: 0.0747189000248909 | [Val] Loss: 0.09357438236474991\n",
      "Epoch 954 -> [Train] Loss: 0.07476536929607391 | [Val] Loss: 0.09377207607030869\n",
      "Epoch 955 -> [Train] Loss: 0.07473617792129517 | [Val] Loss: 0.0934985876083374\n",
      "Epoch 956 -> [Train] Loss: 0.07473105192184448 | [Val] Loss: 0.09347538650035858\n",
      "Epoch 957 -> [Train] Loss: 0.07471803575754166 | [Val] Loss: 0.09361971169710159\n",
      "Epoch 958 -> [Train] Loss: 0.07470867782831192 | [Val] Loss: 0.09351132810115814\n",
      "Epoch 959 -> [Train] Loss: 0.0747079998254776 | [Val] Loss: 0.09337297827005386\n",
      "Epoch 960 -> [Train] Loss: 0.07468092441558838 | [Val] Loss: 0.0936327800154686\n",
      "Epoch 961 -> [Train] Loss: 0.07472893595695496 | [Val] Loss: 0.09370581805706024\n",
      "Epoch 962 -> [Train] Loss: 0.07468817383050919 | [Val] Loss: 0.09358401596546173\n",
      "Epoch 963 -> [Train] Loss: 0.07470248639583588 | [Val] Loss: 0.093275286257267\n",
      "Epoch 964 -> [Train] Loss: 0.07470114529132843 | [Val] Loss: 0.09349530190229416\n",
      "Epoch 965 -> [Train] Loss: 0.0746866911649704 | [Val] Loss: 0.09373272210359573\n",
      "Epoch 966 -> [Train] Loss: 0.07470053434371948 | [Val] Loss: 0.09347102791070938\n",
      "Epoch 967 -> [Train] Loss: 0.07473399490118027 | [Val] Loss: 0.09331845492124557\n",
      "Epoch 968 -> [Train] Loss: 0.07469092309474945 | [Val] Loss: 0.0936131551861763\n",
      "Epoch 969 -> [Train] Loss: 0.07472117245197296 | [Val] Loss: 0.09365571290254593\n",
      "Epoch 970 -> [Train] Loss: 0.07470235973596573 | [Val] Loss: 0.09383883327245712\n",
      "Epoch 971 -> [Train] Loss: 0.0747029185295105 | [Val] Loss: 0.09355202317237854\n",
      "Epoch 972 -> [Train] Loss: 0.07466843724250793 | [Val] Loss: 0.09359273314476013\n",
      "Epoch 973 -> [Train] Loss: 0.07469167560338974 | [Val] Loss: 0.09346403181552887\n",
      "Epoch 974 -> [Train] Loss: 0.07468526065349579 | [Val] Loss: 0.09355942904949188\n",
      "Epoch 975 -> [Train] Loss: 0.07468962669372559 | [Val] Loss: 0.0936003178358078\n",
      "Epoch 976 -> [Train] Loss: 0.07467877864837646 | [Val] Loss: 0.09362802654504776\n",
      "Epoch 977 -> [Train] Loss: 0.07470712810754776 | [Val] Loss: 0.09373831748962402\n",
      "Epoch 978 -> [Train] Loss: 0.0746786892414093 | [Val] Loss: 0.09356813877820969\n",
      "Epoch 979 -> [Train] Loss: 0.07468767464160919 | [Val] Loss: 0.09365616738796234\n",
      "Epoch 980 -> [Train] Loss: 0.07470502704381943 | [Val] Loss: 0.09373364597558975\n",
      "Epoch 981 -> [Train] Loss: 0.07467364519834518 | [Val] Loss: 0.09362617880105972\n",
      "Epoch 982 -> [Train] Loss: 0.07468859851360321 | [Val] Loss: 0.09378377348184586\n",
      "Epoch 983 -> [Train] Loss: 0.07465635985136032 | [Val] Loss: 0.09373895078897476\n",
      "Epoch 984 -> [Train] Loss: 0.07469598203897476 | [Val] Loss: 0.0937836542725563\n",
      "Epoch 985 -> [Train] Loss: 0.07469448447227478 | [Val] Loss: 0.09369432926177979\n",
      "Epoch 986 -> [Train] Loss: 0.07470069080591202 | [Val] Loss: 0.09361692517995834\n",
      "Epoch 987 -> [Train] Loss: 0.0746588483452797 | [Val] Loss: 0.09347184747457504\n",
      "Epoch 988 -> [Train] Loss: 0.07465191930532455 | [Val] Loss: 0.09371865540742874\n",
      "Epoch 989 -> [Train] Loss: 0.07468166947364807 | [Val] Loss: 0.09383440017700195\n",
      "Epoch 990 -> [Train] Loss: 0.07466467469930649 | [Val] Loss: 0.09393823146820068\n",
      "Epoch 991 -> [Train] Loss: 0.07469864934682846 | [Val] Loss: 0.09369546175003052\n",
      "Epoch 992 -> [Train] Loss: 0.07466824352741241 | [Val] Loss: 0.09362473338842392\n",
      "Epoch 993 -> [Train] Loss: 0.0746484100818634 | [Val] Loss: 0.09385818988084793\n",
      "Epoch 994 -> [Train] Loss: 0.07465202361345291 | [Val] Loss: 0.09403427690267563\n",
      "Epoch 995 -> [Train] Loss: 0.07468157261610031 | [Val] Loss: 0.09384683519601822\n",
      "Epoch 996 -> [Train] Loss: 0.07465619593858719 | [Val] Loss: 0.0935470461845398\n",
      "Epoch 997 -> [Train] Loss: 0.07466471195220947 | [Val] Loss: 0.09374020248651505\n",
      "Epoch 998 -> [Train] Loss: 0.07467740029096603 | [Val] Loss: 0.09370477497577667\n",
      "Epoch 999 -> [Train] Loss: 0.07464110106229782 | [Val] Loss: 0.09381433576345444\n",
      "CPU times: user 2h 41min 56s, sys: 52min 50s, total: 3h 34min 46s\n",
      "Wall time: 3h 43min 40s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for epoch in range(EPOCHS):\n",
    "    ## Training\n",
    "    for batch in dst_train_rdy.as_numpy_iterator():\n",
    "        state = train_step(state, batch)\n",
    "        # state = compute_metrics(state=state, batch=batch)\n",
    "        # break\n",
    "\n",
    "    ## Log the metrics\n",
    "    for name, value in state.metrics.compute().items():\n",
    "        metrics_history[f\"train_{name}\"].append(value)\n",
    "    \n",
    "    ## Empty the metrics\n",
    "    state = state.replace(metrics=state.metrics.empty())\n",
    "\n",
    "    ## Evaluation\n",
    "    for batch in dst_val_rdy.as_numpy_iterator():\n",
    "        state = compute_metrics(state=state, batch=batch)\n",
    "        # break\n",
    "    for name, value in state.metrics.compute().items():\n",
    "        metrics_history[f\"val_{name}\"].append(value)\n",
    "    state = state.replace(metrics=state.metrics.empty())\n",
    "    \n",
    "    # wandb.log({\"epoch\": epoch+1, **{name:values[-1] for name, values in metrics_history.items()}})\n",
    "    print(f'Epoch {epoch} -> [Train] Loss: {metrics_history[\"train_loss\"][-1]} | [Val] Loss: {metrics_history[\"val_loss\"][-1]}')\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orbax_checkpointer.save(f\"model-final-{EPOCHS}\", state, save_args=save_args, force=True) # force=True means allow overwritting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating an image\n",
    "\n",
    "PixelCNN is an autorregresive model, so in order to generate a full image we have to generate it one pixel at a time. Good thing is we should be able to `jit` this operation to make it faster!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def obtain_pred(state, inputs): \n",
    "    return state.apply_fn({\"params\": state.params}, inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 56s, sys: 13.6 s, total: 3min 10s\n",
      "Wall time: 2min 45s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "keygen = random.PRNGKey(0)\n",
    "n_imgs_to_generate = 4\n",
    "generated_batch = jnp.zeros(shape=(n_imgs_to_generate,28,28,1))\n",
    "batch, rows, cols, channels = generated_batch.shape\n",
    "\n",
    "for row in range(rows):\n",
    "    for col in range(cols):\n",
    "        for channel in range(channels):\n",
    "            ## 1. Pass the blank images to generate the next (or first) pixel\n",
    "            pred = obtain_pred(state, generated_batch)\n",
    "            pred = nn.sigmoid(pred)\n",
    "            ## 2. Obtain the value of the pixel by sampling from a Bernoulli distribution\n",
    "            pixel = pred[:,row,col,channel]\n",
    "            pixel = random.bernoulli(keygen, p=pixel)*1.\n",
    "            keygen, _ = random.split(keygen)\n",
    "            generated_batch = generated_batch.at[:,row,col,channel].set(pixel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for gen_img in generated_batch:\n",
    "    plt.imshow(gen_img)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @jax.jit\n",
    "# def generate_image():\n",
    "#     n_imgs_to_generate = 1\n",
    "#     generated_batch = jnp.zeros(shape=(n_imgs_to_generate,28,28,1))\n",
    "#     batch, rows, cols, channels = generated_batch.shape\n",
    "\n",
    "#     for row in range(rows):\n",
    "#         for col in range(cols):\n",
    "#             for channel in range(channels):\n",
    "#                 ## 1. Pass the blank images to generate the next (or first) pixel\n",
    "#                 pred = state.apply_fn({\"params\": state.params}, generated_batch)\n",
    "#                 pred = nn.sigmoid(pred)\n",
    "#                 ## 2. Choose the value of the corresponding pixel according to its probability\n",
    "#                 pixel = (pred[:,row,col,channel] > 0.5)*1.\n",
    "#                 generated_batch = generated_batch.at[:,row,col,channel].set(pixel)\n",
    "#     return generated_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# generated_batch = generate_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for gen_img in generated_batch:\n",
    "#     plt.imshow(gen_img)\n",
    "#     plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_gpu_tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
